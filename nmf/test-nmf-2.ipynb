{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from pprint import pprint\n",
    "import scipy.ndimage\n",
    "from IPython.display import display, Audio\n",
    "from activation_learner import ActivationLearner\n",
    "import scipy.signal\n",
    "import logging\n",
    "from unmixdb import UnmixDB\n",
    "from beat_track import beat_stft\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"activation_learner\").setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audios\n",
    "\n",
    "FS = 22050\n",
    "# ref_paths = [\"linear-mix-1.wav\", \"linear-mix-2.wav\"]\n",
    "# mix_path = \"linear-mix.wav\"\n",
    "# ref_paths = [\"original.wav\"]\n",
    "# mix_path = \"original.wav\"\n",
    "# mix_path = \"boucled.wav\"\n",
    "# ref_paths = [\"amen.wav\", \"high.wav\"]\n",
    "# mix_path = \"nuttah.wav\"\n",
    "# ref_paths = [\"nuttah.wav\"]\n",
    "# mix_path= \"nuttah-timestretch.wav\"\n",
    "# refs = [librosa.load(i, sr=FS)[0] for i in ref_paths]\n",
    "# mix = librosa.load(mix_path, sr=FS)[0]\n",
    "\n",
    "## generate chirps\n",
    "# t = np.linspace(0, 2, int(2 * FS))\n",
    "# refs = [\n",
    "    # scipy.signal.chirp(t, 200, 2, 5000) + scipy.signal.chirp(t, 500, 2, 8000),\n",
    "    # scipy.signal.chirp(t, 4000, 2, 800) + scipy.signal.chirp(t, 8000, 2, f1=600),\n",
    "# ]\n",
    "# mix = fade(refs[0], refs[1], int(0.5 * FS))\n",
    "# mix += librosa.load(\"nuttah.wav\", sr=FS)[0][: len(mix)] * 1\n",
    "\n",
    "# mix += librosa.load(\"maya.wav\", sr=FS)[0][: len(mix)] * 0.3\n",
    "# mix /= mix.max()\n",
    "\n",
    "## load unmixdb\n",
    "unmixdb = UnmixDB(\"/data2/anasynth_nonbp/schwarz/abc-dj/data/unmixdb-zenodo\")\n",
    "print(unmixdb.timestretches)\n",
    "print(unmixdb.fxes)\n",
    "mixes = dict(filter(lambda i: i[1].timestretch == \"stretch\" and i[1].fx==\"none\", unmixdb.mixes.items()))\n",
    "x = list(mixes.values())[5]\n",
    "pprint(x)\n",
    "mix = x.audio(sr=FS)\n",
    "refs = [unmixdb.refsongs[track['name']].audio(sr=FS) for track in x.tracks]\n",
    "\n",
    "# ## alignment test\n",
    "# t = np.arange(FS) / FS\n",
    "# sig = np.concatenate(\n",
    "#     [\n",
    "#         np.sin(2 * np.pi * 2000 * t),\n",
    "#         np.sin(2 * np.pi * 4000 * t),\n",
    "#         np.sin(2 * np.pi * 6000 * t),\n",
    "#     ]\n",
    "# )\n",
    "# sig = scipy.signal.chirp(t, 800, t[-1], 2000)\n",
    "# refs = [sig]\n",
    "# mix = np.concatenate([np.ones(3 * FS) * 1e-50, sig])\n",
    "\n",
    "display(Audio(mix, rate=FS))\n",
    "for i, x in enumerate(refs):\n",
    "    print(i)\n",
    "    display(Audio(x, rate=FS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActivationLearner(\n",
    "    mix,\n",
    "    refs,\n",
    "    fs=FS,\n",
    "    additional_dim=0,\n",
    "    win_len = 2**14,\n",
    "    hop_len = 2**12,\n",
    "    n_mels = 512,\n",
    "    col_mag_threshold=1e-8,\n",
    ")\n",
    "losses = []\n",
    "for i in (pbar := tqdm(range(150))):\n",
    "    loss = model.iterate()\n",
    "    losses.append(loss)\n",
    "    pbar.set_description(f\"loss={loss:.2e}\")\n",
    "plt.figure(figsize=(8, 2))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"iter\")\n",
    "plt.title(\"distance\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import soundfile as sf\n",
    "\n",
    "\n",
    "def estimate_tracks():\n",
    "    ret = []\n",
    "    for i in range(len(refs) + 1):\n",
    "        xi = model.reconstruct(i)\n",
    "        ret.append(xi)\n",
    "        print(\"track\", i)\n",
    "        display(Audio(xi, rate=FS))\n",
    "        sf.write(f\"estimated-{i}.wav\", xi / xi.max(), FS)\n",
    "    return ret\n",
    "\n",
    "\n",
    "estimates = estimate_tracks()\n",
    "# sdr, isr, sir, sar = museval.evaluate(\n",
    "#     np.expand_dims(np.vstack(refs), axis=-1),\n",
    "#     np.expand_dims(np.vstack(estimates), axis=-1),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFFT//HLEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_conv = model.nmf.H*scipy.ndimage.convolve1d(model.nmf.H, scipy.signal.get_window(\"hann\", Nx=NFFT//HLEN), axis=1, mode=\"constant\")\n",
    "plt.imshow(H_conv, cmap=CMAP, aspect=\"auto\", origin=\"lower\")\n",
    "plt.show()\n",
    "plt.imshow(model.nmf.H, cmap=CMAP, aspect=\"auto\", origin=\"lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "from pprint import pprint\n",
    "import scipy.ndimage\n",
    "from IPython.display import display, Audio\n",
    "from estimator import ActivationLearner\n",
    "import scipy.signal\n",
    "import logging\n",
    "from unmixdb import UnmixDB\n",
    "from beat_track import beat_stft\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"activation_learner\").setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load audios\n",
    "\n",
    "FS = 22050\n",
    "# input_paths = [\"linear-mix-1.wav\", \"linear-mix-2.wav\", \"linear-mix.wav\"]\n",
    "# input_paths = [\"original.wav\", \"original.wav\"]\n",
    "# input_paths = [\"original.wav\", \"boucled.wav\"]\n",
    "# input_paths = [\"amen.wav\", \"high.wav\", \"nuttah.wav\"]\n",
    "# input_paths = [\"nuttah.wav\", \"nuttah-timestretch.wav\"]\n",
    "# inputs = [librosa.load(i, sr=FS)[0] for i in inputs]\n",
    "\n",
    "\n",
    "## load unmixdb\n",
    "unmixdb = UnmixDB(\"/data2/anasynth_nonbp/schwarz/abc-dj/data/unmixdb-zenodo\")\n",
    "print(unmixdb.timestretches)\n",
    "print(unmixdb.fxes)\n",
    "mixes = dict(filter(lambda i: i[1].timestretch == \"stretch\" and i[1].fx==\"none\", unmixdb.mixes.items()))\n",
    "mix = list(mixes.values())[5]\n",
    "pprint(mix)\n",
    "inputs = [unmixdb.refsongs[track['name']].audio(sr=FS) for track in mix.tracks]\n",
    "inputs.append(mix.audio(sr=FS))\n",
    "\n",
    "for i, mix in enumerate(inputs):\n",
    "    print(i)\n",
    "    display(Audio(mix, rate=FS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ActivationLearner(\n",
    "    inputs,\n",
    "    fs=FS,\n",
    "    additional_dim=0,\n",
    "    win_size = 1.0,\n",
    "    hop_size = 0.25,\n",
    "    n_mels = 512,\n",
    "    col_mag_threshold=1e-8,\n",
    ")\n",
    "model.nmf.H[:200,200:] = 0\n",
    "\n",
    "losses = []\n",
    "for i in (pbar := tqdm(range(150))):\n",
    "    loss = model.iterate()\n",
    "    losses.append(loss)\n",
    "    pbar.set_description(f\"loss={loss:.2e}\")\n",
    "plt.figure(figsize=(8, 2))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"iter\")\n",
    "plt.title(\"distance\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "model.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "for i, v in enumerate(model.volume()):\n",
    "    plt.plot(v, label=f\"track {i}\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import soundfile as sf\n",
    "\n",
    "\n",
    "def estimate_tracks():\n",
    "    ret = []\n",
    "    for i in range(len(refs) + 1):\n",
    "        xi = model.reconstruct(i)\n",
    "        ret.append(xi)\n",
    "        print(\"track\", i)\n",
    "        display(Audio(xi, rate=FS))\n",
    "        sf.write(f\"estimated-{i}.wav\", xi / xi.max(), FS)\n",
    "    return ret"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_nmf\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mixes.synthetic import SyntheticDB\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalties vizualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import activation_learner, plot\n",
    "import librosa\n",
    "import itertools\n",
    "\n",
    "FS = 22050\n",
    "# input_paths = [\"linear-mix-1.wav\", \"linear-mix-2.wav\", \"linear-mix.wav\"]\n",
    "dataset = SyntheticDB()\n",
    "mix = dataset.get_mix(\"stretch\")\n",
    "inputs = mix.as_activation_learner_input()\n",
    "OVERLAP_FACTOR = 4\n",
    "HOP_SIZE = 0.1\n",
    "BETA = 0\n",
    "NMELS = 128\n",
    "# stop conditions\n",
    "ITER_MAX = 1000\n",
    "# logging\n",
    "DIVERGENCE = pytorch_nmf.BetaDivergence(1)\n",
    "PENALTIES = [\n",
    "    pytorch_nmf.L1(),\n",
    "    pytorch_nmf.L2(),\n",
    "    pytorch_nmf.SmoothOverCol(),\n",
    "    pytorch_nmf.SmoothOverRow(),\n",
    "    pytorch_nmf.SmoothDiago(),\n",
    "    pytorch_nmf.Lineness(),\n",
    "]\n",
    "LAMBDAS = [0, 1e-2, 1e-1, 1e1, 1e2]\n",
    "for penalty in PENALTIES:\n",
    "    fig, axs = plt.subplots(4, len(LAMBDAS), sharey=\"row\")\n",
    "    fig.set_size_inches(20, 10)\n",
    "    fig.suptitle(f\"{penalty.__class__.__name__}\")\n",
    "\n",
    "    for p, lambda_ in enumerate(LAMBDAS):\n",
    "        learner = activation_learner.ActivationLearner(\n",
    "            inputs,\n",
    "            fs=FS,\n",
    "            n_mels=NMELS,\n",
    "            win_size=HOP_SIZE * OVERLAP_FACTOR,\n",
    "            hop_size=HOP_SIZE,\n",
    "            divergence=DIVERGENCE,\n",
    "            penalties=[(penalty, lambda_)],\n",
    "            spec_power=1\n",
    "        )\n",
    "        losses = learner.fit(ITER_MAX, loss_every=10)\n",
    "\n",
    "        plot.plot_H(learner.nmf.H.detach().numpy(), learner.split_idx, ax=axs[0, p])\n",
    "        fig.colorbar(axs[0, p].images[0], ax=axs[0, p])\n",
    "        axs[0, p].set_title(f\"$\\\\lambda$ = {lambda_:.2e}\")\n",
    "        axs[1, p].imshow(\n",
    "            lambda_ * penalty.grad_pos(learner.nmf.H).detach().numpy(),\n",
    "            origin=\"lower\",\n",
    "            cmap=\"turbo\",\n",
    "            aspect=\"auto\",\n",
    "        )\n",
    "        fig.colorbar(axs[1, p].images[0], ax=axs[1, p])\n",
    "        axs[1, p].set_title(\"grad pos\")\n",
    "        axs[2, p].imshow(\n",
    "            lambda_ * penalty.grad_neg(learner.nmf.H).detach().numpy(),\n",
    "            origin=\"lower\",\n",
    "            cmap=\"turbo\",\n",
    "            aspect=\"auto\",\n",
    "        )\n",
    "        fig.colorbar(axs[2, p].images[0], ax=axs[2, p])\n",
    "        axs[2, p].set_title(\"grad neg\")\n",
    "        plot.plot_loss_history(losses, ax=axs[3, p])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

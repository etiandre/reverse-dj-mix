@misc{1001tracklists1001TracklistsWorldLeading,
  title = {{{1001Tracklists}} {$\cdot$} {{The World}}'s {{Leading DJ Tracklist}}/{{Playlist Database}}},
  author = {1001Tracklists},
  journal = {1001Tracklists},
  urldate = {2024-03-05},
  abstract = {The world's leading DJ tracklist database ♫ best free mixes, club + festival livesets with playlist ♫ hot EDM songs, fresh charts, news \& more},
  howpublished = {https://www.1001tracklists.com/},
  langid = {american},
  keywords = {dataset},
  file = {/home/etiandre/Zotero/storage/KM4PNPG7/www.1001tracklists.com.html}
}

@inproceedings{aarabiMusicRetilerUsing2018,
  title = {Music Retiler: {{Using NMF2D}} Source Separation for Audio Mosaicing},
  shorttitle = {Music Retiler},
  booktitle = {Proceedings of the {{Audio Mostly}} 2018 on {{Sound}} in {{Immersion}} and {{Emotion}}},
  author = {Aarabi, Hadrien Foroughmand and Peeters, Geoffroy},
  year = {2018},
  month = sep,
  pages = {1--7},
  publisher = {ACM},
  address = {Wrexham United Kingdom},
  doi = {10.1145/3243274.3243299},
  urldate = {2024-03-14},
  abstract = {Musaicing (music mosaicing) aims at reconstructing a target music track by superimposing audio samples selected from a collection. This selection is based on their acoustic similarity to the target. The baseline technique to perform this is concatenative synthesis in which the superposition only occurs in time. Non-Negative Matrix Factorization has also been proposed for this task. In this, a target spectrogram is factorized into an activation matrix and a predefined basis matrix which represents the sample collection. The superposition therefore occurs in time and frequency. However, in both methods the samples used for the reconstruction represent isolated sources (such as bees) and remain unchanged during the musaicing (samples need to be pre-pitch-shifted). This reduces the applicability of these methods. We propose here a variation of the musaicing in which the samples used for the reconstruction are obtained by applying a NMF2D separation algorithm to a music collection (such as a collection of Reggae tracks). Using these separated samples, a second NMF2D algorithm is then used to automatically find the best transposition factors to represent the target. We performed an online perceptual experiment of our method which shows that it outperforms the NMF algorithm when the sources are polyphonic and multi-source.},
  isbn = {978-1-4503-6609-0},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/ZX2D5H6C/Aarabi et Peeters - 2018 - Music retiler Using NMF2D source separation for audio mosaicing.pdf}
}

@article{antunesDynamicalComputationConstrained2017,
  title = {Dynamical Computation of Constrained Flexible Systems Using a Modal {{Udwadia-Kalaba}} Formulation: {{Application}} to Musical Instruments},
  shorttitle = {Dynamical Computation of Constrained Flexible Systems Using a Modal {{Udwadia-Kalaba}} Formulation},
  author = {Antunes, J. and Debut, V.},
  year = {2017},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {141},
  number = {2},
  pages = {764--778},
  issn = {0001-4966, 1520-8524},
  doi = {10.1121/1.4973534},
  urldate = {2024-01-25},
  abstract = {Most musical instruments consist of dynamical subsystems connected at a number of constraining points through which energy flows. For physical sound synthesis, one important difficulty deals with enforcing these coupling constraints. While standard techniques include the use of Lagrange multipliers or penalty methods, in this paper, a different approach is explored, the Udwadia-Kalaba (U-K) formulation, which is rooted on analytical dynamics but avoids the use of Lagrange multipliers. This general and elegant formulation has been nearly exclusively used for conceptual systems of discrete masses or articulated rigid bodies, namely, in robotics. However its natural extension to deal with continuous flexible systems is surprisingly absent from the literature. Here, such a modeling strategy is developed and the potential of combining the U-K equation for constrained systems with the modal description is shown, in particular, to simulate musical instruments. Objectives are twofold: (1) Develop the U-K equation for constrained flexible systems with subsystems modelled through unconstrained modes; and (2) apply this framework to compute string/body coupled dynamics. This example complements previous work [Debut, Antunes, Marques, and Carvalho, Appl. Acoust. 108, 3--18 (2016)] on guitar modeling using penalty methods. Simulations show that the proposed technique provides similar results with a significant improvement in computational efficiency.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/YNIE3K3G/Antunes et Debut - 2017 - Dynamical computation of constrained flexible syst.pdf}
}

@article{aspillagaMIXMERECOMMENDATIONSYSTEM2011,
  title = {{{MIXME}}: {{A RECOMMENDATION SYSTEM FOR DJS}}},
  author = {Aspillaga, Felipe X and Cobb, Jonathan and Chuan, Ching-Hua},
  year = {2011},
  journal = {ISMIR},
  langid = {english},
  keywords = {unread},
  file = {/home/etiandre/stage/biblio/mixme 10.1.1.374.9075.pdf}
}

@article{atiamHighQualityMusical,
  title = {High {{Quality Musical Audio Source Separation}}},
  author = {Atiam, Master},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/5M7KA9HU/Atiam - High Quality Musical Audio Source Separation.pdf}
}

@misc{AudioLabsLetIt,
  title = {{{AudioLabs}} - {{Let}} It {{Bee}} - {{Towards NMF-inspired Audio Mosaicing}}},
  urldate = {2024-03-08},
  howpublished = {https://www.audiolabs-erlangen.de/resources/MIR/2015-ISMIR-LetItBee},
  keywords = {alignment},
  file = {/home/etiandre/Zotero/storage/RVEV3STG/2015-ISMIR-LetItBee.html}
}

@article{badeauAnalyseSynthese,
  title = {Analyse-{{Synth{\`e}se}}},
  author = {Badeau, Roland},
  file = {/home/etiandre/Zotero/storage/BLAPP2B3/Analyse-synthese.pdf;/home/etiandre/Zotero/storage/CRVFRGK6/cours1.pdf;/home/etiandre/Zotero/storage/I24TX8ZD/cours2.pdf}
}

@article{badeauApplicationsOuvertures,
  title = {Applications et Ouvertures},
  author = {Badeau, Roland},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/BYKEEPBZ/Badeau - Applications et ouvertures.pdf}
}

@article{badeauFondamentauxTraitementSignal,
  title = {{Fondamentaux de traitement du signal}},
  author = {Badeau, Roland},
  langid = {french},
  file = {/home/etiandre/Zotero/storage/7L4W7PEW/Badeau - Fondamentaux de traitement du signal.pdf;/home/etiandre/Zotero/storage/9UXC3BZL/cours1.pdf;/home/etiandre/Zotero/storage/EGMFDJ87/exercices2.pdf;/home/etiandre/Zotero/storage/FTGBI7FM/exercices3.pdf;/home/etiandre/Zotero/storage/H2HKC7BJ/cours2.pdf;/home/etiandre/Zotero/storage/JBTDUF36/cours3.pdf;/home/etiandre/Zotero/storage/LNZG3CT6/exercices1.pdf;/home/etiandre/Zotero/storage/QGJMHG95/corrige2.pdf;/home/etiandre/Zotero/storage/UGHGXRRV/corrigé1.pdf}
}

@article{badeauMethodesHauteResolution,
  title = {{M{\'e}thodes {\`a} haute r{\'e}solution}},
  author = {Badeau, Roland},
  langid = {french},
  file = {/home/etiandre/Zotero/storage/98ACFJ4T/Badeau - Corrigé des exercices sur les méthodes à haute résolution.pdf;/home/etiandre/Zotero/storage/HLZWG28T/Badeau - M´ethodes `a haute r´esolution.pdf;/home/etiandre/Zotero/storage/TFBX8SF6/Badeau - Exercices sur les méthodes à haute résolution.pdf}
}

@article{badeauNonnegativeMatrixFactorization,
  title = {Non-Negative {{Matrix Factorization}}},
  author = {Badeau, Roland},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/3HWIYZHX/Badeau - Non-negative Matrix Factorization.pdf}
}

@article{barchiesiReverseEngineeringMix2010,
  title = {Reverse {{Engineering}} of a {{Mix}}},
  author = {Barchiesi, Daniele and Reiss, Joshua},
  year = {2010},
  journal = {J. Audio Eng. Soc.},
  volume = {58},
  number = {7},
  langid = {english},
  keywords = {unread},
  file = {/home/etiandre/stage/biblio/BarchiesiReiss-ReverseEngineeringofaMix.pdf}
}

@article{bernardesPerceptuallyMotivatedHarmonicCompatibility2017,
  title = {A {{Perceptually-Motivated Harmonic Compatibility Method}} for {{Music Mixing}}},
  author = {Bernardes, Gilberto and Davies, Matthew E P and Guedes, Carlos},
  year = {2017},
  abstract = {We present a method for assisting users in the process of music mashup creation. Our main contribution is a harmonic compatibility metric between musical audio samples which combines existing perceptual relatedness (i.e., chroma vectors or key a nity) and consonance approaches. Our harmonic compatibility metric is derived from Tonal Interval Space indicators, which we adapt to robustly describe the harmonic content of musical audio. Additional attributes of key, rhythmic (density), and spectral content are computed from musical audio to enhance the compatibility representation of a sample collection in an interactive visualization. An evaluation of our harmonic compatibility method shows that it complies with the principles embodied in Western tonal harmony to a greater extent than previous approaches.},
  langid = {english},
  file = {/home/etiandre/stage/biblio/BernardesDaviesGuedes -cmmr2017- perceptually harmonic mixing paper_62.pdf}
}

@article{bittnerAutomaticPlaylistSequencing2017,
  title = {Automatic {{Playlist Sequencing}} and {{Transitions}}},
  author = {Bittner, Rachel M and Gu, Minwei and Hernandez, Gandalf and Humphrey, Eric J and Jehan, Tristan and McCurry, P Hunter and Montecchio, Nicola},
  year = {2017},
  journal = {ISMIR},
  abstract = {Professional music curators and DJs artfully arrange and mix recordings together to create engaging, seamless, and cohesive listening experiences, a craft enjoyed by audiences around the world. The average listener, however, lacks both the time and the skill necessary to create comparable experiences, despite access to same source material. As a result, user-generated listening sessions often lack the sophistication popularized by modern artists, e.g. tracks are played in their entirety with little or no thought given to their ordering. To these ends, this paper presents methods for automatically sequencing existing playlists and adding DJ-style crossfade transitions between tracks: the former is modeled as a graph traversal problem, and the latter as an optimization problem. Our approach is motivated by an analysis of listener data on a large music catalog, and subjectively evaluated by professional curators.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/QZLNSTUT/Bittner et al. - 2017 - AUTOMATIC PLAYLIST SEQUENCING AND TRANSITIONS.pdf}
}

@article{bittnerDeepSalienceRepresentations,
  title = {Deep {{Salience Representations}} for {{F0 Estimation}} in {{Polyphonic Music}}},
  author = {Bittner, Rachel M and McFee, Brian and Salamon, Justin and Li, Peter and Bello, Juan P},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/ERD4HLRH/Bittner et al. - Deep Salience Representations for F0 Estimation in Polyphonic Music.pdf;/home/etiandre/Zotero/storage/RQ6WTPYY/Bittner et al. - 2017 - DEEP SALIENCE REPRESENTATIONS FOR F0 ESTIMATION IN POLYPHONIC MUSIC.pdf}
}

@misc{buchinComputingContinuousDynamic2023,
  title = {Computing {{Continuous Dynamic Time Warping}} of {{Time Series}} in {{Polynomial Time}}},
  author = {Buchin, Kevin and Nusser, Andr{\'e} and Wong, Sampson},
  year = {2023},
  month = apr,
  number = {arXiv:2203.04531},
  eprint = {2203.04531},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.04531},
  urldate = {2024-03-13},
  abstract = {Dynamic Time Warping is arguably the most popular similarity measure for time series, where we define a time series to be a one-dimensional polygonal curve. The drawback of Dynamic Time Warping is that it is sensitive to the sampling rate of the time series. The Fr{\textbackslash}'echet distance is an alternative that has gained popularity, however, its drawback is that it is sensitive to outliers. Continuous Dynamic Time Warping (CDTW) is a recently proposed alternative that does not exhibit the aforementioned drawbacks. CDTW combines the continuous nature of the Fr{\textbackslash}'echet distance with the summation of Dynamic Time Warping, resulting in a similarity measure that is robust to sampling rate and to outliers. In a recent experimental work of Brankovic et al., it was demonstrated that clustering under CDTW avoids the unwanted artifacts that appear when clustering under Dynamic Time Warping and under the Fr{\textbackslash}'echet distance. Despite its advantages, the major shortcoming of CDTW is that there is no exact algorithm for computing CDTW, in polynomial time or otherwise. In this work, we present the first exact algorithm for computing CDTW of one-dimensional curves. Our algorithm runs in time \$O(n{\textasciicircum}5)\$ for a pair of one-dimensional curves, each with complexity at most \$n\$. In our algorithm, we propagate continuous functions in the dynamic program for CDTW, where the main difficulty lies in bounding the complexity of the functions. We believe that our result is an important first step towards CDTW becoming a practical similarity measure between curves.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computational Geometry},
  file = {/home/etiandre/Zotero/storage/W5N2IQA6/Buchin et al. - 2023 - Computing Continuous Dynamic Time Warping of Time Series in Polynomial Time.pdf;/home/etiandre/Zotero/storage/CZ6LA672/2203.html}
}

@inproceedings{burloiuOnlineTempoTracker2016,
  title = {An {{Online Tempo Tracker}} for {{Automatic Accompaniment}} Based on {{Audio-to-audio Alignment}} and {{Beat Tracking}}},
  booktitle = {Sound and {{Music Computing Conference}}},
  author = {Burloiu, Grigore},
  year = {2016},
  month = aug,
  address = {Hamburg, Germany},
  urldate = {2024-03-06},
  abstract = {We approach a specific scenario in real-time performance following for automatic accompaniment, where a relative tempo value is derived from the deviation between a live target performance and a stored reference, to drive the play-back speed of an accompaniment track. We introduce a system which combines an online alignment process with a beat tracker. The former aligns the target performance to the reference without resorting to any symbolic information. The latter utilises the beat positions detected in the accompaniment, reference and target tracks to (1) improve the robustness of the alignment-based tempo model and (2) take over the tempo computation in segments when the alignment error is likely high. While other systems exist that handle structural deviations and mistakes in a performance, the portions of time where the aligner is attempting to find the correct hypothesis can produce erratic tempo values. Our proposed system, publicly available as a Max/MSP external object, addresses this problem.},
  keywords = {unread},
  file = {/home/etiandre/Zotero/storage/F26G573R/Burloiu - 2016 - An Online Tempo Tracker for Automatic Accompaniment based on Audio-to-audio Alignment and Beat Track.pdf}
}

@misc{C3S3_MusicAppTempoCurve,
  title = {{{C3S3}}\_{{MusicAppTempoCurve}}},
  urldate = {2024-03-13},
  howpublished = {https://www.audiolabs-erlangen.de/resources/MIR/FMP/C3/C3S3\_MusicAppTempoCurve.html},
  file = {/home/etiandre/Zotero/storage/VF7L9PNF/C3S3_MusicAppTempoCurve.html}
}

@article{cardosoMIXTAPEDIRECTIONBASEDNAVIGATION2016,
  title = {{{MIXTAPE}}: {{DIRECTION-BASED NAVIGATION IN LARGE MEDIA COLLECTIONS}}},
  author = {Cardoso, Joao Paulo V and Pontello, Luciana Fujii and Holanda, Pedro H F and Guilherme, Bruno and Goussevskaia, Olga},
  year = {2016},
  journal = {New York City},
  abstract = {In this work we explore the increasing demand for novel user interfaces to navigate large media collections. We implement a scalable data structure to store and retrieve similarity information and propose a novel navigation framework that uses geometric vector operations and real-time user feedback to direct the outcome. In particular, we implement this framework in the domain of music. To evaluate the effectiveness of the navigation process, we propose an automatic evaluation framework, based on synthetic user profiles, which allows to quickly simulate and compare navigation paths using different algorithms and datasets. Moreover, we perform a real user study. To do that, we developed and launched Mixtape 1 , a simple web application that allows users to create playlists by providing real-time feedback through liking and skipping patterns.},
  langid = {english},
  keywords = {unread},
  file = {/home/etiandre/stage/biblio/CardosoEtAl -ismir2016- mixtape media navigation 155_Paper.pdf}
}

@book{chaigneAcousticsMusicalInstruments2016,
  title = {Acoustics of {{Musical Instruments}}},
  author = {Chaigne, Antoine and Kergomard, Jean},
  year = {2016},
  series = {Modern {{Acoustics}} and {{Signal Processing}}},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4939-3679-3},
  urldate = {2023-11-07},
  isbn = {978-1-4939-3677-9 978-1-4939-3679-3},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/7IA7IPJS/Chaigne et Kergomard - 2016 - Acoustics of Musical Instruments.pdf}
}

@inproceedings{chenAutomaticDJTransitions2022,
  title = {Automatic {{DJ Transitions}} with {{Differentiable Audio Effects}} and {{Generative Adversarial Networks}}},
  booktitle = {{{ICASSP}} 2022 - 2022 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chen, Bo-Yu and Hsu, Wei-Han and Liao, Wei-Hsiang and Ram{\'i}rez, Marco A. Mart{\'i}nez and Mitsufuji, Yuki and Yang, Yi-Hsuan},
  year = {2022},
  month = may,
  pages = {466--470},
  issn = {2379-190X},
  doi = {10.1109/ICASSP43922.2022.9746663},
  urldate = {2024-03-06},
  abstract = {A central task of a Disc Jockey (DJ) is to create a mixset of music with seamless transitions between adjacent tracks. In this paper, we explore a data-driven approach that uses a generative adversarial network to create the song transition by learning from real-world DJ mixes. The generator uses two differentiable digital signal processing components, an equalizer (EQ) and a fader, to mix two tracks selected by a data generation pipeline. The generator has to set the parameters of the EQs and fader in such a way that the resulting mix resembles real mixes created by human DJ, as judged by the discriminator counterpart. Result of a listening test shows that the model can achieve competitive results compared with a number of baselines.},
  keywords = {Acoustics,audio effects,Conferences,deep learning,differentiable signal processing,Digital signal processing,DJ mix,Equalizers,generative adversarial network,Generative adversarial networks,Generators,Pipelines,unread},
  file = {/home/etiandre/Zotero/storage/98CLWVD8/Chen et al. - 2022 - Automatic DJ Transitions with Differentiable Audio Effects and Generative Adversarial Networks.pdf;/home/etiandre/Zotero/storage/RNJM39FU/9746663.html}
}

@article{cichockiFastLocalAlgorithms2009,
  title = {Fast {{Local Algorithms}} for {{Large Scale Nonnegative Matrix}} and {{Tensor Factorizations}}},
  author = {Cichocki, Andrzej and Phan, Anh-Huy},
  year = {2009},
  month = mar,
  journal = {IEICE Transactions},
  volume = {92-A},
  pages = {708--721},
  doi = {10.1587/transfun.E92.A.708},
  abstract = {Nonnegative matrix factorization (NMF) and its extensions such as Nonnegative Tensor Factorization (NTF) have become prominent techniques for blind sources separation (BSS), analysis of image databases, data mining and other information retrieval and clustering applications. In this paper we propose a family of efficient algorithms for NMF/NTF, as well as sparse nonnegative coding and representation, that has many potential applications in computational neuroscience, multi-sensory processing, compressed sensing and multidimensional data analysis. We have developed a class of optimized local algorithms which are referred to as Hierarchical Alternating Least Squares (HALS) algorithms. For these purposes, we have performed sequential constrained minimization on a set of squared Euclidean distances. We then extend this approach to robust cost functions using the alpha and beta divergences and derive flexible update rules. Our algorithms are locally stable and work well for NMF-based blind source separation (BSS) not only for the over-determined case but also for an under-determined (over-complete) case (i.e., for a system which has less sensors than sources) if data are sufficiently sparse. The NMF learning rules are extended and generalized for N-th order nonnegative tensor factorization (NTF). Moreover, these algorithms can be tuned to different noise statistics by adjusting a single parameter. Extensive experimental results confirm the accuracy and computational performance of the developed algorithms, especially, with usage of multi-layer hierarchical NMF approach [3].},
  file = {/home/etiandre/Zotero/storage/GER7H75R/Cichocki et Phan - 2009 - Fast Local Algorithms for Large Scale Nonnegative Matrix and Tensor Factorizations.pdf}
}

@article{cliffHangDJAutomatic2000,
  title = {Hang the {{DJ}}: {{Automatic Sequencing}} and {{Seamless Mixing}} of {{Dance-Music Tracks}}},
  author = {Cliff, Dave},
  year = {2000},
  month = aug,
  journal = {Hp Laboratories Technical Report Hpl},
  volume = {104},
  abstract = {Many radio stations and night-clubs employ Disk-Jockeys (DJs) to provide a continuous stream or ``mix'' of music, built from a sequence of individual song-tracks. In the last decade, commercial pre-recorded compilation CDs of DJ mixes have become a booming market. DJs exercise skill in deciding an appropriate sequence of tracks and in mixing 'seamlessly' from one track to the next. Online access to large-scale archives of digitized music via automated music information retrieval systems offers users the possibility of discovering many new songs they like, but the majority of consumers are unlikely to want to learn the DJ skills of sequencing and mixing. This paper describes an automatic method by which compilations of dance-music can be sequenced and seamlessly mixed by computer, with minimal user involvement. The user may specify a selection of tracks, and may give a qualitative indication of the type of mix required. The resultant mix can be presented as a continuous single digital audio file, whether for burning to CD or for play-out from a virtual jukebox or personalized virtual radio station.},
  langid = {english},
  keywords = {unread},
  file = {/home/etiandre/stage/biblio/hang the dj HPL-2000-104.pdf}
}

@article{CodebarresEAN2024,
  title = {{Code-barres EAN}},
  year = {2024},
  month = jan,
  journal = {Wikip{\'e}dia},
  urldate = {2024-03-05},
  abstract = {Le code EAN (European Article Numbering ; aussi appel{\'e} International Article Number ou IAN) est un code-barres utilis{\'e} par le commerce et l'industrie conform{\'e}ment aux sp{\'e}cifications d'EAN International, organisme aujourd'hui remplac{\'e} par GS1. Il est connu en France sous le nom de Gencode, {\`a} tort, puisque Gencode {\'e}tait le nom de l'organisme fran{\c c}ais charg{\'e} de sa r{\'e}gulation nationale.Historiquement, le code EAN est d{\'e}riv{\'e} du code universel des produits (UPC, ou Universal Product Code) d{\'e}velopp{\'e} dans les ann{\'e}es 1970 par George Laurer. La d{\'e}nomination actuelle est GTIN (Global Trade Item Number).},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {french},
  annotation = {Page Version ID: 211449278}
}

@article{colonelMusicProductionBehaviour2023,
  title = {Music {{Production Behaviour Modelling}}},
  author = {Colonel, J.},
  year = {2023},
  urldate = {2024-03-06},
  abstract = {The new millennium has seen an explosion of computational approaches to the study of music production, due in part to the decreasing cost of computation and the increase of digital music production techniques. The rise of digital recording equipment, MIDI, digital audio workstations (DAWs), and software plugins for audio effects led to the digital capture of various processes in music production. This discretization of traditionally analogue methods allowed for the development of intelligent music production, which uses machine learning to numerically characterize and automate portions of the music production process. One algorithm from the field referred to as ``reverse engineering a multitrack mix'' can recover the audio effects processing used to transform a multitrack recording into a mixdown in the absence of information about how the mixdown was achieved. This thesis improves on this method of reverse engineering a mix by leveraging recent advancements in machine learning for audio. Using the differentiable digital signal processing paradigm, greybox modules for gain, panning, equalisation, artificial reverberation, memoryless waveshaping distortion, and dynamic range compression are presented. These modules are then connected in a mixing chain and are optimized to learn the effects used in a given mixdown. Both objective and perceptual metrics are presented to measure the performance of these various modules in isolation and within a full mixing chain. Ultimately a fully differentiable mixing chain is presented that outperforms previously proposed methods to reverse engineer a mix. Directions for future work are proposed to improve characterization of multitrack mixing behaviours.},
  langid = {english},
  annotation = {Accepted: 2023-06-27T10:00:29Z},
  file = {/home/etiandre/Zotero/storage/992DHT6M/Colonel - 2023 - Music Production Behaviour Modelling.pdf}
}

@inproceedings{cramerLookListenLearn2019,
  title = {Look, {{Listen}}, and {{Learn More}}: {{Design Choices}} for {{Deep Audio Embeddings}}},
  shorttitle = {Look, {{Listen}}, and {{Learn More}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Cramer, Aurora Linh and Wu, Ho-Hsiang and Salamon, Justin and Bello, Juan Pablo},
  year = {2019},
  month = may,
  pages = {3852--3856},
  publisher = {IEEE},
  address = {Brighton, UK},
  doi = {10.1109/ICASSP.2019.8682475},
  urldate = {2024-03-05},
  abstract = {A considerable challenge in applying deep learning to audio classification is the scarcity of labeled data. An increasingly popular solution is to learn deep audio embeddings from large audio collections and use them to train shallow classifiers using small labeled datasets. Look, Listen, and Learn (L3-Net) is an embedding trained through self-supervised learning of audio-visual correspondence in videos as opposed to other embeddings requiring labeled data. This framework has the potential to produce powerful out-of-the-box embeddings for downstream audio classification tasks, but has a number of unexplained design choices that may impact the embeddings' behavior. In this paper we investigate how L3-Net design choices impact the performance of downstream audio classifiers trained with these embeddings. We show that audio-informed choices of input representation are important, and that using sufficient data for training the embedding is key. Surprisingly, we find that matching the content for training the embedding to the downstream task is not beneficial. Finally, we show that our best variant of the L3-Net embedding outperforms both the VGGish and SoundNet embeddings, while having fewer parameters and being trained on less data. Our implementation of the L3-Net embedding model as well as pre-trained models are made freely available online.},
  isbn = {978-1-4799-8131-1},
  langid = {english},
  file = {/home/etiandre/stage/biblio/CramerWuSalamonBello -icassp2019- looklistenlearnmore openl3.pdf}
}

@misc{CueNation,
  title = {{{CueNation}}},
  urldate = {2024-03-25},
  howpublished = {https://cuenation.com/},
  file = {/home/etiandre/Zotero/storage/JU832WZM/cuenation.com.html}
}

@article{davidHarmonicNoiseDecomposition2006,
  title = {Harmonic plus Noise Decomposition: Time-Frequency Reassignment versus a Subspace Based Method},
  author = {David, Bertrand and Emiya, Valentin and Badeau, Roland and Grenier, Yves},
  year = {2006},
  abstract = {This work deals with the Harmonic+Noise decomposition and, as targeted application, to extract transient background noise surrounded by a signal having a strong harmonic content (speech for instance). In that perspective, a method based on the reassigned spectrum and a High Resolution subspace tracker are compared, both on simulations and in a more realistic manner. The reassignment re-localizes the time-frequency energy around a given pair (analysis time index, analysis frequency bin) while the High Resolution method benefits from a characterization of the signal in terms of a space spanned by the harmonic content and a space spanned by the stochastic content. Both methods are adaptive and the estimations are updated from a sample to the next.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/LGP8VVTW/David et al. - 2006 - Harmonic plus noise decomposition time-frequency .pdf}
}

@article{deguchiDetectionEnhancementLine,
  title = {Detection and {{Enhancement}} of {{Line Structures}} in an {{Image}} by {{Anisotropic Diffusion}}},
  author = {Deguchi, Koichiro and Izumitani, Tadahiro and Hontani, Hidekata},
  abstract = {This paper describes a method to enhance line structures in a gray level image. For this purpose, we blur the image using anisotropic gaussian filters along the directions of each line structures. In a line structure region the gradients of image gray levels have a uniform direction. To find such line structures, we evaluate the uniformity of the directions of the local gradients. Before this evaluation, we need to smooth out small structures to obtain line directions. We, first, blur the given image by a set of gaussian filters. The variance of the gaussian filter which maximizes the uniformity of the local gradient directions is detected position by position. Then, the line directions in the image are obtained from this blurred image. Finally, we blur the image using anisotropic filter again along the directions, and enhance every line structure.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/UUPE4H5S/Deguchi et al. - Detection and Enhancement of Line Structures in an Image by Anisotropic Diﬀusion.pdf}
}

@article{demanAnalysisEvaluationAudio2014,
  title = {An {{Analysis}} and {{Evaluation}} of {{Audio Features}} for {{Multitrack Music Mixtures}}},
  author = {De Man, Brecht and Leonard, Brett and King, Richard and Reiss, Joshua},
  year = {2014},
  journal = {ISMIR},
  keywords = {unread},
  file = {/home/etiandre/stage/biblio/DeManLeonardKingReiss-ismir2014-analysis and evaluation of audio features for multitrack music mixtures Published.pdf}
}

@misc{DiscoverMusicSamples,
  title = {Discover {{Music}} via {{Samples}}, {{Cover Songs}} and {{Remixes}} {\textbar} {{WhoSampled}}},
  urldate = {2024-04-29},
  abstract = {Dig deeper into music at the ultimate database of sampled music, cover songs and remixes},
  howpublished = {https://www.whosampled.com/},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/3NXW2MH5/www.whosampled.com.html}
}

@article{driedgerLetItBee2015,
  title = {Let {{It Bee}} -- {{Towards NMF-Inspired Audio Mosaicing}}},
  author = {Driedger, Jonathan and Pratzlich, Thomas},
  year = {2015},
  journal = {ISMIR},
  abstract = {A swarm of bees buzzing ``Let it be'' by the Beatles or the wind gently howling the romantic ``Gute Nacht'' by Schubert -- these are examples of audio mosaics as we want to create them. Given a target and a source recording, the goal of audio mosaicing is to generate a mosaic recording that conveys musical aspects (like melody and rhythm) of the target, using sound components taken from the source. In this work, we propose a novel approach for automatically generating audio mosaics with the objective to preserve the source's timbre in the mosaic. Inspired by algorithms for non-negative matrix factorization (NMF), our idea is to use update rules to learn an activation matrix that, when multiplied with the spectrogram of the source recording, resembles the spectrogram of the target recording. However, when applying the original NMF procedure, the resulting mosaic does not adequately reflect the source's timbre. As our main technical contribution, we propose an extended set of update rules for the iterative learning procedure that supports the development of sparse diagonal structures in the activation matrix. We show how these structures better retain the source's timbral characteristics in the resulting mosaic.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/N5737W2M/Driedger et Pratzlich - 2015 - LET IT BEE – TOWARDS NMF-INSPIRED AUDIO MOSAICING.pdf}
}

@misc{EANSearchOrg,
  title = {{{EAN-Search}}.Org},
  urldate = {2024-03-05},
  abstract = {Search our EAN database with over 460 million products by EAN, UPC, ISBN or product name},
  howpublished = {https://www.ean-search.org/},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/T9QCDA3H/www.ean-search.org.html}
}

@inproceedings{ewertHighResolutionAudio2009,
  title = {High Resolution Audio Synchronization Using Chroma Onset Features},
  booktitle = {2009 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}}},
  author = {Ewert, Sebastian and Muller, Meinard and Grosche, Peter},
  year = {2009},
  month = apr,
  pages = {1869--1872},
  publisher = {IEEE},
  address = {Taipei, Taiwan},
  doi = {10.1109/ICASSP.2009.4959972},
  urldate = {2024-03-13},
  abstract = {The general goal of music synchronization is to automatically align the multiple information sources such as audio recordings, MIDI files, or digitized sheet music related to a given musical work. In computing such alignments, one typically has to face a delicate tradeoff between robustness and accuracy. In this paper, we introduce novel audio features that combine the high temporal accuracy of onset features with the robustness of chroma features. We show how previous synchronization methods can be extended to make use of these new features. We report on experiments based on polyphonic Western music demonstrating the improvements of our proposed synchronization framework.},
  isbn = {978-1-4244-2353-8},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/AHGR6JGU/Ewert et al. - 2009 - High resolution audio synchronization using chroma onset features.pdf}
}

@article{fevotteAlgorithmsNonnegativeMatrix2011,
  title = {Algorithms for Nonnegative Matrix Factorization with the Beta-Divergence},
  author = {F{\'e}votte, C{\'e}dric and Idier, J{\'e}r{\^o}me},
  year = {2011},
  journal = {Neural Computation},
  volume = {23},
  number = {9},
  pages = {2421--2456},
  urldate = {2024-04-22},
  abstract = {This paper describes algorithms for nonnegative matrix factorization (NMF) with the {$\beta$}-divergence ({$\beta$}-NMF). The {$\beta$}-divergence is a family of cost functions parametrized by a single shape parameter {$\beta$} that takes the Euclidean distance, the Kullback-Leibler divergence and the Itakura-Saito divergence as special cases ({$\beta$} = 2, 1, 0 respectively). The proposed algorithms are based on a surrogate auxiliary function (a local majorization of the criterion function). We first describe a majorizationminimization (MM) algorithm that leads to multiplicative updates, which differ from standard heuristic multiplicative updates by a {$\beta$}-dependent power exponent. The monotonicity of the heuristic algorithm can however be proven for {$\beta$} {$\in$} (0, 1) using the proposed auxiliary function. Then we introduce the concept of majorization-equalization (ME) algorithm which produces updates that move along constant level sets of the auxiliary function and lead to larger steps than MM. Simulations on synthetic and real data illustrate the faster convergence of the ME approach. The paper also describes how the proposed algorithms can be adapted to two common variants of NMF: penalized NMF (i.e., when a penalty function of the factors is added to the criterion function) and convex-NMF (when the dictionary is assumed to belong to a known subspace).},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  file = {/home/etiandre/Zotero/storage/VRB2TVAD/Févotte et Idier - 2011 - Algorithms for nonnegative matrix factorization with the beta-divergence.pdf}
}

@inproceedings{fevotteMajorizationminimizationAlgorithmSmooth2011,
  title = {Majorization-Minimization Algorithm for Smooth {{Itakura-Saito}} Nonnegative Matrix Factorization},
  booktitle = {2011 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Fevotte, Cedric},
  year = {2011},
  month = may,
  pages = {1980--1983},
  publisher = {IEEE},
  address = {Prague, Czech Republic},
  doi = {10.1109/ICASSP.2011.5946898},
  urldate = {2024-04-19},
  abstract = {Nonnegative matrix factorization (NMF) with the ItakuraSaito divergence has proven efficient for audio source separation and music transcription, where the signal power spectrogram is factored into a ``dictionary'' matrix times an ``activation'' matrix. Given the nature of audio signals it is expected that the activation coefficients exhibit smoothness along time frames. This may be enforced by penalizing the NMF objective function with an extra term reflecting smoothness of the activation coefficients. We propose a novel regularization term that solves some deficiencies of our previous work and leads to an efficient implementation using a majorizationminimization procedure.},
  isbn = {978-1-4577-0538-0},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/IEZWN6H8/Fevotte - 2011 - Majorization-minimization algorithm for smooth Itakura-Saito nonnegative matrix factorization.pdf}
}

@article{fevotteNonnegativeMatrixFactorization2009,
  title = {Nonnegative {{Matrix Factorization}} with the {{Itakura-Saito Divergence}}: {{With Application}} to {{Music Analysis}}},
  shorttitle = {Nonnegative {{Matrix Factorization}} with the {{Itakura-Saito Divergence}}},
  author = {F{\'e}votte, C{\'e}dric and Bertin, Nancy and Durrieu, Jean-Louis},
  year = {2009},
  month = mar,
  journal = {Neural Computation},
  volume = {21},
  number = {3},
  pages = {793--830},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2008.04-08-771},
  urldate = {2024-04-25},
  abstract = {This letter presents theoretical, algorithmic, and experimental results about nonnegative matrix factorization (NMF) with the Itakura-Saito (IS) divergence. We describe how IS-NMF is underlaid by a well-defined statistical model of superimposed gaussian components and is equivalent to maximum likelihood estimation of variance parameters. This setting can accommodate regularization constraints on the factors through Bayesian priors. In particular, inverse-gamma and gamma Markov chain priors are considered in this work. Estimation can be carried out using a space-alternating generalized expectation-maximization (SAGE) algorithm; this leads to a novel type of NMF algorithm, whose convergence to a stationary point of the IS cost function is guaranteed.             We also discuss the links between the IS divergence and other cost functions used in NMF, in particular, the Euclidean distance and the generalized Kullback-Leibler (KL) divergence. As such, we describe how IS-NMF can also be performed using a gradient multiplicative algorithm (a standard algorithm structure in NMF) whose convergence is observed in practice, though not proven.             Finally, we report a furnished experimental comparative study of Euclidean-NMF, KL-NMF, and IS-NMF algorithms applied to the power spectrogram of a short piano sequence recorded in real conditions, with various initializations and model orders. Then we show how IS-NMF can successfully be employed for denoising and upmix (mono to stereo conversion) of an original piece of early jazz music. These experiments indicate that IS-NMF correctly captures the semantics of audio and is better suited to the representation of music signals than NMF with the usual Euclidean and KL costs.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/G2C8XU2V/Févotte et al. - 2009 - Nonnegative Matrix Factorization with the Itakura-Saito Divergence With Application to Music Analys.pdf}
}

@incollection{fevotteSingleChannelAudioSource2018,
  title = {Single-{{Channel Audio Source Separation}} with {{NMF}}: {{Divergences}}, {{Constraints}} and {{Algorithms}}},
  shorttitle = {Single-{{Channel Audio Source Separation}} with {{NMF}}},
  booktitle = {Audio {{Source Separation}}},
  author = {F{\'e}votte, C{\'e}dric and Vincent, Emmanuel and Ozerov, Alexey},
  editor = {Makino, Shoji},
  year = {2018},
  pages = {1--24},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-73031-8_1},
  urldate = {2024-04-19},
  abstract = {Spectral decomposition by nonnegative matrix factorisation (NMF) has become state-of-the-art practice in many audio signal processing tasks, such as source separation, enhancement or transcription. This chapter reviews the fundamentals of NMF-based audio decomposition, in unsupervised and informed settings. We formulate NMF as an optimisation problem and discuss the choice of the measure of fit. We present the standard majorisation-minimisation strategy to address optimisation for NMF with common {$\beta$} -divergence, a family of measures of fit that takes the quadratic cost, the generalised Kullback-Leibler divergence and the ItakuraSaito divergence as special cases. We discuss the reconstruction of time-domain components from the spectral factorisation and present common variants of NMFbased spectral decomposition: supervised and informed settings, regularised versions, temporal models.},
  isbn = {978-3-319-73030-1 978-3-319-73031-8},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/GGZA4DBJ/Févotte et al. - 2018 - Single-Channel Audio Source Separation with NMF Divergences, Constraints and Algorithms.pdf}
}

@incollection{fevotteSingleChannelAudioSource2018a,
  title = {Single-{{Channel Audio Source Separation}} with {{NMF}}: {{Divergences}}, {{Constraints}} and {{Algorithms}}},
  shorttitle = {Single-{{Channel Audio Source Separation}} with {{NMF}}},
  booktitle = {Audio {{Source Separation}}},
  author = {F{\'e}votte, C{\'e}dric and Vincent, Emmanuel and Ozerov, Alexey},
  editor = {Makino, Shoji},
  year = {2018},
  pages = {1--24},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-73031-8_1},
  urldate = {2024-04-19},
  abstract = {Spectral decomposition by nonnegative matrix factorisation (NMF) has become state-of-the-art practice in many audio signal processing tasks, such as source separation, enhancement or transcription. This chapter reviews the fundamentals of NMF-based audio decomposition, in unsupervised and informed settings. We formulate NMF as an optimisation problem and discuss the choice of the measure of fit. We present the standard majorisation-minimisation strategy to address optimisation for NMF with common {$\beta$} -divergence, a family of measures of fit that takes the quadratic cost, the generalised Kullback-Leibler divergence and the ItakuraSaito divergence as special cases. We discuss the reconstruction of time-domain components from the spectral factorisation and present common variants of NMFbased spectral decomposition: supervised and informed settings, regularised versions, temporal models.},
  isbn = {978-3-319-73030-1 978-3-319-73031-8},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/KVTJC94Q/Févotte et al. - 2018 - Single-Channel Audio Source Separation with NMF Divergences, Constraints and Algorithms.pdf}
}

@incollection{fevotteTemporalExtensionsNonnegative2018,
  title = {Temporal {{Extensions}} of {{Nonnegative Matrix Factorization}}},
  booktitle = {Audio {{Source Separation}} and {{Speech Enhancement}}},
  author = {F{\'e}votte, C{\'e}dric and Smaragdis, Paris and Mohammadiha, Nasser and Mysore, Gautham J.},
  editor = {Vincent, Emmanuel and Virtanen, Tuomas and Gannot, Sharon},
  year = {2018},
  month = sep,
  edition = {1},
  pages = {161--187},
  publisher = {Wiley},
  doi = {10.1002/9781119279860.ch9},
  urldate = {2024-04-22},
  isbn = {978-1-119-27986-0},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/5FMRFAP6/Févotte et al. - 2018 - Temporal Extensions of Nonnegative Matrix Factorization.pdf}
}

@inproceedings{fujioSystemMixingSongs2003,
  title = {A {{System}} of {{Mixing Songs}} for {{Automatic DJ Performance}} Using {{Genetic Programming}}},
  booktitle = {6th {{Asian Design International Conference}}},
  author = {Fujio, Tsuyoshi and Shiizuka, Hisao},
  year = {2003},
  abstract = {A method of mixing songs automatically like DJs is proposed, where ``mix'' means to join some songs in optional timing. It is shown that everyone can enjoy mixing songs easily because any difficulties of a DJ's technique are not found with this system. In addition, the songs, that are created using this system, are the user's original mixes because this system is designed with considerations including human KANSEI (sensibility).},
  langid = {english},
  keywords = {unread},
  file = {/home/etiandre/stage/biblio/Fujio -asianDesignConf2003- automatic DJ performance using genetic programming 496.pdf}
}

@article{gillisSparseUniqueNonnegative,
  title = {Sparse and {{Unique Nonnegative Matrix Factorization Through Data Preprocessing}}},
  author = {Gillis, Nicolas},
  abstract = {Nonnegative matrix factorization (NMF) has become a very popular technique in machine learning because it automatically extracts meaningful features through a sparse and part-based representation. However, NMF has the drawback of being highly ill-posed, that is, there typically exist many different but equivalent factorizations. In this paper, we introduce a completely new way to obtaining more well-posed NMF problems whose solutions are sparser. Our technique is based on the preprocessing of the nonnegative input data matrix, and relies on the theory of M-matrices and the geometric interpretation of NMF. This approach provably leads to optimal and sparse solutions under the separability assumption of Donoho and Stodden (2003), and, for rank-three matrices, makes the number of exact factorizations finite. We illustrate the effectiveness of our technique on several image data sets.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/EGSV4KUW/Gillis - Sparse and Unique Nonnegative Matrix Factorization Through Data Preprocessing.pdf}
}

@misc{grufkorkGrufkorkRkbx_osc2024,
  title = {Grufkork/Rkbx\_osc},
  author = {{grufkork}},
  year = {2024},
  month = mar,
  urldate = {2024-03-25},
  abstract = {A tiny tool for sending Rekordbox timing information over OSC},
  keywords = {audiovisual,osc,rekordbox}
}

@inproceedings{gunnarssonFrequencyAnalysisUsing2004,
  title = {Frequency Analysis Using Non-Uniform Sampling with Application to Active Queue Management},
  booktitle = {2004 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Gunnarsson, F. and Gustafsson, F.},
  year = {2004},
  month = may,
  volume = {2},
  pages = {ii-581},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2004.1326324},
  urldate = {2024-03-11},
  abstract = {In many real-time applications, sample values and time stamps are delivered in pairs, where sampling times are non-uniform. Frequency analysis using non-uniform data occurs in various real life problems and embedded systems, such as vibrational analysis in cars and control of packet network queue lengths. Our contribution is first to overview different ways to approximate the Fourier transform, and secondly to give analytical expressions for how non-uniform sampling affects these approximations. The results are expressed in terms of frequency windows describing how a single frequency in the continuous time signal is smeared out in the frequency domain, or, more precisely, in the expected value of the Fourier transform approximation.},
  keywords = {Control systems,Data analysis,Embedded system,Fourier transforms,Frequency,Jitter,Queueing analysis,Sampling methods,Signal sampling,Spline},
  file = {/home/etiandre/Zotero/storage/4GBAC5BY/Gunnarsson et Gustafsson - 2004 - Frequency analysis using non-uniform sampling with application to active queue management.pdf}
}

@article{hanAccurateRapidContinuous2018,
  title = {An Accurate and Rapid Continuous Wavelet Dynamic Time Warping Algorithm for End-to-End Mapping in Ultra-Long Nanopore Sequencing},
  author = {Han, Renmin and Li, Yu and Gao, Xin and Wang, Sheng},
  year = {2018},
  month = sep,
  journal = {Bioinformatics},
  volume = {34},
  number = {17},
  pages = {i722-i731},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/bty555},
  urldate = {2024-03-13},
  abstract = {Long-reads, point-of-care and polymerase chain reaction-free are the promises brought by nanopore sequencing. Among various steps in nanopore data analysis, the end-to-end mapping between the raw electrical current signal sequence and the reference expected signal sequence serves as the key building block to signal labeling, and the following signal visualization, variant identification and methylation detection. One of the classic algorithms to solve the signal mapping problem is the dynamic time warping (DTW). However, the ultra-long nanopore sequencing and an order of magnitude difference in the sampling speed complexify the scenario and make the classical DTW infeasible to solve the problem.Here, we propose a novel multi-level DTW algorithm, continuous wavelet DTW (cwDTW), based on continuous wavelet transforms with different scales of the two signal sequences. Our algorithm starts from low-resolution wavelet transforms of the two sequences, such that the transformed sequences are short and have similar sampling rates. Then the peaks and nadirs of the transformed sequences are extracted to form feature sequences with similar lengths, which can be easily mapped by the original DTW. Our algorithm then recursively projects the warping path from a lower-resolution level to a higher-resolution one by building a context-dependent boundary and enabling a constrained search for the warping path in the latter. Comprehensive experiments on two real nanopore datasets on human and on Pandoraea pnomenusa demonstrate the efficiency and effectiveness of the proposed algorithm. In particular, cwDTW can gain remarkable acceleration with tiny loss of the alignment accuracy. On the real nanopore datasets, cwDTW can finish an alignment task in few seconds, which is about 3000 times faster than the original DTW. By successfully applying cwDTW on the tasks of signal labeling and ultra-long sequence comparison, we further demonstrate the power and applicability of cwDTW.Our program is available at https://github.com/realbigws/cwDTW.Supplementary data are available at Bioinformatics online.},
  file = {/home/etiandre/Zotero/storage/KBP8A9LV/Han et al. - 2018 - An accurate and rapid continuous wavelet dynamic time warping algorithm for end-to-end mapping in ul.pdf}
}

@phdthesis{hansenAcousticsPerformanceDJ2010,
  title = {The Acoustics and Performance of {{DJ}} Scratching},
  author = {Hansen, Kjetil Falkenberg},
  year = {2010},
  abstract = {This thesis focuses on the analysis and modeling of scratching, in other words, the DJ (disk jockey) practice of using the turntable as a musical instrument. There has been experimental use of turntables as musical instruments since their invention, but the use is now mainly ascribed to the musical genre hip-hop and the playing style known as scratching. Scratching has developed to become a skillful instrument-playing practice with complex musical output performed by DJs. The impact on popular music culture has been significant, and for many, the DJ set-up of turntables and a mixer is now a natural instrument choice for undertaking a creative music activity. Six papers are included in this thesis, where the first three approach the acoustics and performance of scratching, and the second three approach scratch modeling and the DJ interface. Additional studies included here expand on the scope of the papers. For the acoustics and performance studies, DJs were recorded playing both demonstrations of standard performance techniques, and expressive performances on sensor-equipped instruments. Analysis of the data revealed that there are both differences and commonalities in playing strategies between musicians, and between expressive intentions. One characteristic feature of scratching is the range of standard playing techniques, but in performances it seems DJs vary the combination of playing techniques more than the rendering of these techniques. The third study describes some of the acoustic parameters of typical scratch improvisations and looks at which musical parameters are typically used for expressive performances. Extracted acoustic and performance parameters from the data show the functional ranges within which DJs normally play.},
  langid = {english},
  school = {KTH Royal Institute of Technology},
  keywords = {unread},
  file = {/home/etiandre/Zotero/storage/C96B8SJS/Hansen - The acoustics and performance of DJ scratching.pdf}
}

@article{hansenDJScratchingPerformance2003,
  title = {{{DJ Scratching Performance Techniques}}: {{Analysis}} and {{Synthesis}}},
  author = {Hansen, Kjetil Falkenberg and Bresin, Roberto},
  year = {2003},
  journal = {Proc. Stockholm Music Acoust. Conf},
  pages = {693--696},
  abstract = {Scratching is a popular way of making music, turning the DJ into a musician. Normally scratching is done using a vinyl record, a turntable and a mixer. Vinyl manipulation is built up by a number of specialized techniques that have been analysed in a previous study. The present study has two main objectives. First is to better understand and model turntable scratching as performed by DJs. Second is to design a gesture controller for physical sound models, i.e. models of friction sounds. We attached sensors to a DJ equipment set-up. Then a DJ was asked to perform typical scratch gestures both isolated and in a musical context, i.e. as in a real performance. He also was asked to play with different emotions: sad, angry, happy and fearful. A model of the techniques used by the DJ was built based on the analysis of the collected data. The implementation of the model has been done in pd. The Radio Baton, with specially adapted gesture controllers, has been used for controlling the model. The system has been played by professional DJs in concerts.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/CJ852FYH/Hansen et Bresin - 2003 - DJ SCRATCHING PERFORMANCE TECHNIQUES ANALYSIS AND SYNTHESIS.pdf}
}

@article{helieVolterraSeriesState2010,
  title = {Volterra {{Series}} and {{State Transformation}} for {{Real-Time Simulations}} of {{Audio Circuits Including Saturations}}: {{Application}} to the {{Moog Ladder Filter}}},
  shorttitle = {Volterra {{Series}} and {{State Transformation}} for {{Real-Time Simulations}} of {{Audio Circuits Including Saturations}}},
  author = {Helie, Thomas},
  year = {2010},
  month = may,
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {18},
  number = {4},
  pages = {747--759},
  issn = {1558-7916},
  doi = {10.1109/TASL.2009.2035211},
  urldate = {2023-11-07},
  abstract = {Volterra series are known to be efficient to represent weakly nonlinear systems and the first distortions. Their truncated versions allow one to derive realizations (in the sense of system theory) leading to networks composed of linear filters, sums, and instantaneous products of signals, without instantaneous feedback loops. Nevertheless, if saturation phenomena arise, truncating the series at low order is not sufficient and the convergence can also be lost. In this paper, the case of the Moog ladder filter is investigated. Low-cost simulations based on realizations of Volterra series are given. Their limitations with respect to the amplitude of input signals are exhibited. Methods to increase the validity range and to improve the efficiency of Volterra series expansions are detailed on a single stage of the filter. In particular, changes of states based on the difference between the original state and predictors (parameterized by a tunable delay ) yield satisfying results. The digital simulation of this system preserves the properties mentioned above. It includes two delay lines (where the delay can be chosen to be one sample) and nonlinear static functions given by the method.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/DNTCJRCQ/Helie - 2010 - Volterra Series and State Transformation for Real-.pdf}
}

@misc{HIDMapping,
  title = {{{HID Mapping}}},
  journal = {GitHub},
  urldate = {2024-03-14},
  abstract = {Mixxx is Free DJ software that gives you everything you need to perform live mixes. - mixxxdj/mixxx},
  howpublished = {https://github.com/mixxxdj/mixxx/wiki/HID-Mapping},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/NJEZ9XGG/Hid-Mapping.html}
}

@misc{HttpsPureRoyalholloway,
  title = {{{https://pure.royalholloway.ac.uk/ws/portalfiles/portal/17846086/FinalVersion.pdf}}},
  urldate = {2024-03-25},
  howpublished = {https://pure.royalholloway.ac.uk/ws/portalfiles/portal/17846086/FinalVersion.pdf},
  file = {/home/etiandre/Zotero/storage/VUQ5Q5DX/FinalVersion.pdf}
}

@misc{HttpsWwwMixotic,
  title = {{{https://www.mixotic.net/}}},
  urldate = {2024-03-06},
  howpublished = {https://www.mixotic.net/},
  keywords = {dataset},
  file = {/home/etiandre/Zotero/storage/SIB54MBD/www.mixotic.net.html}
}

@misc{ImprovingDJMixReverse,
  title = {Improving {{DJ-Mix Reverse Engineering}} ({{Master}} 2023 2024)},
  urldate = {2024-03-05},
  howpublished = {https://www-master.ufr-info-p6.jussieu.fr/2023/Improving-DJ-Mix-Reverse},
  keywords = {alignment,identification},
  file = {/home/etiandre/Zotero/storage/ZC3C2NX9/Improving-DJ-Mix-Reverse.html}
}

@misc{IntroductionDynamicTime,
  title = {An Introduction to {{Dynamic Time Warping}}},
  urldate = {2024-03-13},
  howpublished = {https://rtavenar.github.io/blog/dtw.html},
  file = {/home/etiandre/Zotero/storage/R3V42364/dtw.html}
}

@article{ishizakiFullAutomaticDJMixing2009,
  title = {Full-{{Automatic DJ Mixing System}} with {{Optimal Tempo Adjustment Based}} on {{Measurement Function}} of {{User Discomfort}}},
  author = {Ishizaki, Hiromi and Hoashi, Keiichiro and Takishima, Yasuhiro},
  year = {2009},
  journal = {ISMIR},
  abstract = {This paper proposes an automatic DJ mixing method that can automate the processes of real world DJs and describes a prototype for a fully automatic DJ mix-like playing system. Our goal is to achieve a fully automatic DJ mixing system that can preserve overall user comfort level during DJ mixing.},
  langid = {english},
  keywords = {unread},
  file = {/home/etiandre/stage/biblio/IshizakiHoashiTakishima -ismir2009- FULL-AUTOMATIC DJ MIXING SYSTEM OPTIMAL TEMPO ADJUSTMENT USER DISCOMFORT PS1-14.pdf}
}

@article{jenseniusProceedingsInternationalConference2011,
  title = {Proceedings of the {{International Conference}} on {{New Interfaces}} for {{Musical Expression}} - 30 {{May}} - 1 {{June}} 2011},
  author = {Jensenius, Alexander Refsum},
  year = {2011},
  abstract = {We present BeatJockey, a prototype interface which makes use of Audio Mosaicing (AM), beat-tracking and machine learning techniques, for supporting Diskjockeys (DJs) by proposing them new ways of interaction with the songs on the DJ's playlist. This prototype introduces a new paradigm to DJing in which the user has the capability to mix songs interacting with beat-units that accompany the DJ's mix. For this type of interaction, the system suggests song slices taken from songs selected from a playlist, which could go well with the beats of whatever master song is being played. In addition the system allows the synchronization of multiple songs, thus permitting flexible, coherent and rapid progressions in the DJ's mix. BeatJockey uses the Reactable, a musical tangible user interface (TUI), and it has been designed to be used by all DJs regardless of their level of expertise, as the system helps the novice while bringing new creative opportunities to the expert.},
  langid = {english},
  keywords = {unread},
  file = {/home/etiandre/stage/biblio/beatjockey -nime2011- 10.1.1.353.5910.pdf}
}

@article{kellEmpiricalAnalysisTrack2013,
  title = {Empirical {{Analysis}} of {{Track Selection}} and {{Ordering}} in {{Electronic Dance Music Using Audio Feature Extraction}}},
  author = {Kell, Thor and Tzanetakis, George},
  year = {2013},
  journal = {ISMIR},
  abstract = {Disc jockeys are in some ways the ultimate experts at selecting and playing recorded music for an audience, especially in the context of dance music. In this work, we empirically investigate factors affecting track selection and ordering using DJ-created mixes of electronic dance music. We use automatic content-based analysis and discuss the implications of our findings to playlist generation and ordering. Timbre appears to be an important factor when selecting tracks and ordering tracks, and track order itself matters, as shown by statistically significant differences in the transitions between the original order and a shuffled version. We also apply this analysis to ordering heuristics and suggest that the standard playlist generation model of returning tracks in order of decreasing similarity to the initial track may not be optimal, at least in the context of track ordering for electronic dance music.},
  langid = {english},
  keywords = {unread},
  file = {/home/etiandre/stage/biblio/KellTzanetakis-ismir2013-Empirical analysis of track selection and ordering in electronic  210_Paper.pdf}
}

@article{kimAutomaticDjMix2017,
  title = {Automatic {{Dj Mix Generation Using Highlight Detection}}},
  author = {Kim, Adrian and Park, Soram and Park, Jangyeon and Ha, Jung-Woo and Kwon, Taegyun and Nam, Juhan},
  year = {2017},
  journal = {Proc. ISMIR, late-breaking demo paper},
  abstract = {We demonstrate a fully automated system that generates DJ mixes from an arbitrary pool of audio tracks. Our goal is to offer a framework which automatically provides a high-quality endless mix of musical tracks without user supervision, given a seed track. To achieve this, we follow a filter and rank-based approach which selects the bestmatching clip of a song to mix with the given seed track. We applied state-of-the-art musical information retrieval (MIR) methods including deep learning-based approaches such as automatic highlight extraction and key-tempo estimation for extracting cue points, choosing transition methods, and selecting audio clips. The results show that the proposed system can automatically generate competitive DJ mix contents plausible for users to enjoy.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/S37DVLFJ/Kim et al. - AUTOMATIC DJ MIX GENERATION USING HIGHLIGHT DETECTION.pdf}
}

@article{kimComputationalAnalysisRealWorld2020,
  title = {A {{Computational Analysis}} of {{Real-World DJ Mixes Using Mix-to-Track Subsequence Alignment}}},
  author = {Kim, Taejun and Choi, Minsuk and Sacks, Evan and Yang, Yi-Hsuan and Nam, Juhan},
  year = {2020},
  journal = {ISMIR},
  abstract = {A DJ mix is a sequence of music tracks concatenated seamlessly, typically rendered for audiences in a live setting by a DJ on stage. As a DJ mix is produced in a studio or the live version is recorded for music streaming services, computational methods to analyze DJ mixes, for example, extracting track information or understanding DJ techniques, have drawn research interests. Many of previous works are, however, limited to identifying individual tracks in a mix or segmenting it, and the sizes of the datasets are usually small. In this paper, we provide an in-depth analysis of DJ music by aligning a mix to its original music tracks. We set up the subsequence alignment such that the audio features are less sensitive to the tempo or key change of the original track in a mix. This approach provides temporally tight mix-to-track matching from which we can obtain cue-points, transition length, mix segmentation, and musical changes in DJ performance. Using 1,557 mixes from 1001Tracklists including 13,728 tracks and 20,765 transitions, we conduct the proposed analysis and show a wide range of statistics, which may elucidate the creative process of DJ music making.},
  langid = {english},
  keywords = {alignment,dataset,extraction},
  file = {/home/etiandre/stage/biblio/KimChoiSacksYangNam-ismir2020-computational analysis of real-world dj mixes 000352.pdf}
}

@article{kimJointEstimationFader2022,
  title = {Joint {{Estimation}} of {{Fader}} and {{Equalizer Gains}} of {{DJ Mixers Using Convex Optimization}}},
  author = {Kim, Taejun and Yang, Yi-Hsuan and Nam, Juhan},
  year = {2022},
  journal = {DAFx},
  abstract = {Disc jockeys (DJs) use audio effects to make a smooth transition from one song to another. There have been attempts to computationally analyze the creative process of seamless mixing. However, only a few studies estimated fader or equalizer (EQ) gains controlled by DJs. In this study, we propose a method that jointly estimates time-varying fader and EQ gains so as to reproduce the mix from individual source tracks. The method approximates the equalizer filters with a linear combination of a fixed equalizer filter and a constant gain to convert the joint estimation into a convex optimization problem. For the experiment, we collected a new DJ mix dataset that consists of 5,040 real-world DJ mixes with 50,742 transitions, and evaluated the proposed method with a mix reconstruction error. The result shows that the proposed method estimates the time-varying fader and equalizer gains more accurately than existing methods and simple baselines.},
  langid = {english},
  keywords = {eq,extraction,fader},
  file = {/home/etiandre/stage/biblio/KimYangNam-dafx2022- fader eq estimation convex optimization.pdf}
}

@inproceedings{kimReverseEngineeringTransitionRegions2021,
  title = {Reverse-{{Engineering The Transition Regions}} of {{Real-World DJ Mixes}} Using {{Sub-band Analysis}} with {{Convex Optimization}}},
  booktitle = {{{NIME}} 2021},
  author = {Kim, Taejun and Yang, Yi-Hsuan and Nam, Juhan},
  year = {2021},
  month = jun,
  publisher = {PubPub},
  address = {Shanghai, China},
  doi = {10.21428/92fbeb44.4b2fc7b9},
  urldate = {2024-03-05},
  abstract = {The basic role of DJs is creating a seamless sequence of music tracks. In order to make the DJ mix a single continuous audio stream, DJs control various audio effects on a DJ mixer system particularly in the transition region between one track and the next track and modify the audio signals in terms of volume, timbre, tempo, and other musical elements. There have been research efforts to imitate the DJ mixing techniques but they are mainly rule-based approaches based on domain knowledge. In this paper, we propose a method to analyze the DJ mixer control from real-world DJ mixes toward a data-driven approach to imitate the DJ performance. Specifically, we estimate the mixing gain trajectories between the two tracks using sub-band analysis with constrained convex optimization. We evaluate the method by reconstructing the original tracks using the two source tracks and the gain estimate, and show that the proposed method outperforms the linear crossfading as a baseline and the single-band analysis. A listening test from the survey of 14 participants also confirms that our proposed method is superior among them. A web demo is available at this link.},
  langid = {english},
  keywords = {eq,fader},
  file = {/home/etiandre/stage/biblio/KimYangNam-nime2021-Reverse-engineering transition regions of real-world DJ mixes sub-band analysis convex optimization.pdf}
}

@misc{kondratievTimkondratievRekordJog2024,
  title = {Timkondratiev/{{RekordJog}}},
  author = {Kondratiev, Tim},
  year = {2024},
  month = mar,
  urldate = {2024-03-25},
  abstract = {Enable jog wheels and pitch control on third-party controllers to function with Rekordbox 6.},
  copyright = {MIT},
  keywords = {dj,music}
}

@misc{kongPANNsLargeScalePretrained2020,
  title = {{{PANNs}}: {{Large-Scale Pretrained Audio Neural Networks}} for {{Audio Pattern Recognition}}},
  shorttitle = {{{PANNs}}},
  author = {Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Yuxuan and Wang, Wenwu and Plumbley, Mark D.},
  year = {2020},
  month = aug,
  number = {arXiv:1912.10211},
  eprint = {1912.10211},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  urldate = {2024-03-05},
  abstract = {Audio pattern recognition is an important research topic in the machine learning area, and includes several tasks such as audio tagging, acoustic scene classification, music classification, speech emotion classification and sound event detection. Recently, neural networks have been applied to tackle audio pattern recognition problems. However, previous systems are built on specific datasets with limited durations. Recently, in computer vision and natural language processing, systems pretrained on large-scale datasets have generalized well to several tasks. However, there is limited research on pretraining systems on large-scale datasets for audio pattern recognition. In this paper, we propose pretrained audio neural networks (PANNs) trained on the large-scale AudioSet dataset. These PANNs are transferred to other audio related tasks. We investigate the performance and computational complexity of PANNs modeled by a variety of convolutional neural networks. We propose an architecture called Wavegram-Logmel-CNN using both log-mel spectrogram and waveform as input feature. Our best PANN system achieves a state-of-the-art mean average precision (mAP) of 0.439 on AudioSet tagging, outperforming the best previous system of 0.392. We transfer PANNs to six audio pattern recognition tasks, and demonstrate state-of-the-art performance in several of those tasks. We have released the source code and pretrained models of PANNs: https://github.com/qiuqiangkong/audioset\_tagging\_cnn.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/etiandre/stage/biblio/KongCaoWangPlumbley-arxiv2020-PANNs 1912.10211.pdf}
}

@article{lecarrouLowcostHighprecisionMeasurement2014,
  title = {A Low-Cost High-Precision Measurement Method of String Motion},
  author = {Le Carrou, J.-L. and Chadefaux, D. and Seydoux, L. and Fabre, B.},
  year = {2014},
  month = aug,
  journal = {Journal of Sound and Vibration},
  volume = {333},
  number = {17},
  pages = {3881--3888},
  issn = {0022460X},
  doi = {10.1016/j.jsv.2014.04.023},
  urldate = {2024-01-29},
  abstract = {The high-precision measurement of a string's motion requires either the use of an expensive apparatus or the development of a dedicated system. In this paper, a cheaper alternative based on opto-switch sensors combined with a suitable calibration is proposed. A sensitivity model requiring only two straightforward preliminary measurements to determine the parameters is presented. A comparison on a bench test between the opto-switch sensor and a high-speed camera has been performed. Results indicate that the calibrated opto-switch provides more accurate measurements of the string's motion in quasi-static as well as in dynamic states.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/LE62YF2G/Le Carrou et al. - 2014 - A low-cost high-precision measurement method of st.pdf}
}

@misc{lefevreOnlineAlgorithmsNonnegative2011,
  title = {Online Algorithms for {{Nonnegative Matrix Factorization}} with the {{Itakura-Saito}} Divergence},
  author = {Lef{\`e}vre, Augustin and Bach, Francis and F{\'e}votte, C{\'e}dric},
  year = {2011},
  month = jun,
  number = {arXiv:1106.4198},
  eprint = {1106.4198},
  primaryclass = {stat},
  publisher = {arXiv},
  urldate = {2024-04-19},
  abstract = {Nonnegative matrix factorization (NMF) is now a common tool for audio source separation. When learning NMF on large audio databases, one major drawback is that the complexity in time is O(F KN ) when updating the dictionary (where (F, N ) is the dimension of the input power spectrograms, and K the number of basis spectra), thus forbidding its application on signals longer than an hour. We provide an online algorithm with a complexity of O(F K) in time and memory for updates in the dictionary. We show on audio simulations that the online approach is faster for short audio signals and allows to analyze audio signals of several hours.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Statistics - Machine Learning},
  file = {/home/etiandre/Zotero/storage/K96WJ7BU/Lefèvre et al. - 2011 - Online algorithms for Nonnegative Matrix Factorization with the Itakura-Saito divergence.pdf}
}

@article{leplatMultiResolutionBetaDivergenceNMF2022,
  title = {Multi-{{Resolution Beta-Divergence NMF}} for {{Blind Spectral Unmixing}}},
  author = {Leplat, Valentin and Gillis, Nicolas and F{\'e}votte, C{\'e}dric},
  year = {2022},
  month = apr,
  journal = {Signal Processing},
  volume = {193},
  eprint = {2007.03893},
  primaryclass = {cs, eess},
  pages = {108428},
  issn = {01651684},
  doi = {10.1016/j.sigpro.2021.108428},
  urldate = {2024-04-10},
  abstract = {Many datasets are obtained as a resolution trade-off between two adversarial dimensions; for example between the frequency and the temporal resolutions for the spectrogram of an audio signal, and between the number of wavelengths and the spatial resolution for a hyper/multi-spectral image. To perform blind source separation using observations with different resolutions, a standard approach is to use coupled nonnegative matrix factorizations (NMF). Most previous works have focused on the least squares error measure, which is the \${\textbackslash}beta\$-divergence for \${\textbackslash}beta = 2\$. In this paper, we formulate this multi-resolution NMF problem for any \${\textbackslash}beta\$-divergence, and propose an algorithm based on multiplicative updates (MU). We show on numerical experiments that the MU are able to obtain high resolutions in both dimensions on two applications: (1) blind unmixing of audio spectrograms: to the best of our knowledge, this is the first time a coupled NMF model is used in this context, and (2) the fusion of hyperspectral and multispectral images: we show that the MU compete favorable with state-of-the-art algorithms in particular in the presence of non-Gaussian noise.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Electrical Engineering and Systems Science - Image and Video Processing,Electrical Engineering and Systems Science - Signal Processing},
  file = {/home/etiandre/Zotero/storage/E5AN6RTE/Leplat et al. - 2022 - Multi-Resolution Beta-Divergence NMF for Blind Spectral Unmixing.pdf;/home/etiandre/Zotero/storage/7BRRJV8N/2007.html}
}

@misc{liLooPyResearchFriendlyMix2023,
  title = {{{LooPy}}: {{A Research-Friendly Mix Framework}} for {{Music Information Retrieval}} on {{Electronic Dance Music}}},
  shorttitle = {{{LooPy}}},
  author = {Li, Xinyu},
  year = {2023},
  month = may,
  number = {arXiv:2305.01051},
  eprint = {2305.01051},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  urldate = {2024-03-05},
  abstract = {Music information retrieval (MIR) has gone through an explosive development with the advancement of deep learning in recent years. However, music genres like electronic dance music (EDM) has always been relatively less investigated compared to others. Considering its wide range of applications, we present a Python package for automated EDM audio generation as an infrastructure for MIR for EDM songs, to mitigate the difficulty of acquiring labelled data. It is a convenient tool that could be easily concatenated to the end of many symbolic music generation pipelines. Inside this package, we provide a framework to build professional-level templates that could render a well-produced track from specified melody and chords, or produce massive tracks given only a specific key by our probabilistic symbolic melody generator. Experiments show that our mixes could achieve the same quality of the original reference songs produced by world-famous artists, with respect to both subjective and objective criteria. Our code is accessible in this repository1 and the official site of the project is also online2.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {audio generation,Computer Science - Information Retrieval,Computer Science - Machine Learning,Computer Science - Multimedia,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/etiandre/stage/biblio/Li -arxiv2023- loopy mix framework 2305.01051.pdf}
}

@inproceedings{listgartenMultipleAlignmentContinuous2004,
  title = {Multiple {{Alignment}} of {{Continuous Time Series}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Listgarten, Jennifer and Neal, Radford and Roweis, Sam and Emili, Andrew},
  year = {2004},
  volume = {17},
  publisher = {MIT Press},
  urldate = {2024-03-08},
  abstract = {Multiple realizations of continuous-valued time series from a stochastic            process often contain systematic variations in rate and amplitude. To            leverage the information contained in such noisy replicate sets, we need            to align them in an appropriate way (for example, to allow the data to be            properly combined by adaptive averaging). We present the Continuous            Profile Model (CPM), a generative model in which each observed time            series is a non-uniformly subsampled version of a single latent trace, to            which local rescaling and additive noise are applied. After unsupervised            training, the learned trace represents a canonical, high resolution fusion            of all the replicates. As well, an alignment in time and scale of each            observation to this trace can be found by inference in the model. We            apply CPM to successfully align speech signals from multiple speakers            and sets of Liquid Chromatography-Mass Spectrometry proteomic data.},
  keywords = {alignment},
  file = {/home/etiandre/Zotero/storage/RQRN88YL/Listgarten et al. - 2004 - Multiple Alignment of Continuous Time Series.pdf}
}

@article{liuNovelDistanceMeasure2024,
  title = {A Novel Distance Measure Based on Dynamic Time Warping to Improve Time Series Classification},
  author = {Liu, Yutao and Zhang, Yong-An and Zeng, Ming and Zhao, Jie},
  year = {2024},
  month = jan,
  journal = {Information Sciences},
  volume = {656},
  pages = {119921},
  issn = {0020-0255},
  doi = {10.1016/j.ins.2023.119921},
  urldate = {2024-03-06},
  abstract = {Dynamic time warping (DTW) is the most widely used method to evaluate the similarity between time series. However, the DTW distance only takes into account the difference in amplitude, but does not reflect the time distortion information between them. In this paper, we propose a novel time similarity metric, called the time distortion coefficient, based on the DTW warping path to quantify the time distortion between time series. It is able to characterize the type and degree of time distortion between two time series at each point. By summing the absolute values of the time distortion coefficients, the overall time distortion is introduced to quantify time distortion between two time series. For the Nearest Neighbor (NN) based time series classification, a fusional similarity measure combining the DTW distance and the overall time distortion measure is proposed, which is able to evaluate the similarity in both amplitude and time domains. The experimental results conducted on the UCR time series classification archive datasets demonstrate that the proposed fusional similarity measure can significantly improve the classification accuracy of the 1-NN classifier with only a small amount of additional computational cost compared to the DTW distance and other metrics.},
  keywords = {Dynamic time warping,One nearest neighbor classification,Similarity measure,Time series classification,unread},
  file = {/home/etiandre/Zotero/storage/6U3UJNJS/S0020025523015062.html}
}

@misc{LiveTracklistDiscoverDJ,
  title = {{{LiveTracklist}} - {{Discover DJ Tracklists}} and {{Playlists}}},
  urldate = {2024-03-06},
  abstract = {Explore the latest tracklists, playlists, and unreleased tracks from your favorite artists, including EDM festivals, live sets, DJ mixes, radio shows, and podcasts.},
  howpublished = {https://www.livetracklist.com/},
  langid = {american},
  keywords = {dataset},
  file = {/home/etiandre/Zotero/storage/6A5L2DUH/www.livetracklist.com.html}
}

@inproceedings{lopez-serranoFindingDrumBreaks2018,
  title = {Finding {{Drum Breaks}} in {{Digital Music Recordings}}},
  booktitle = {Music {{Technology}} with {{Swing}}},
  author = {{L{\'o}pez-Serrano}, Patricio and Dittmar, Christian and M{\"u}ller, Meinard},
  editor = {Aramaki, Mitsuko and Davies, Matthew E. P. and {Kronland-Martinet}, Richard and Ystad, S{\o}lvi},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {111--122},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-01692-0_8},
  abstract = {DJs and producers of sample-based electronic dance music (EDM) use breakbeats as an essential building block and rhythmic foundation for their artistic work. The practice of reusing and resequencing sampled drum breaks critically influenced modern musical genres such as hip hop, drum'n'bass, and jungle. While EDM artists have primarily sourced drum breaks from funk, soul, and jazz recordings from the 1960s to 1980s, they can potentially be sampled from music of any genre. In this paper, we introduce and formalize the task of automatically finding suitable drum breaks in music recordings. By adapting an approach previously used for singing voice detection, we establish a first baseline for drum break detection. Besides a quantitative evaluation, we discuss benefits and limitations of our procedure by considering a number of challenging examples.},
  isbn = {978-3-030-01692-0},
  langid = {english},
  keywords = {Audio classification,Breakbeat,Drum break,Electronic dance music,Machine learning,Music information retrieval},
  file = {/home/etiandre/Zotero/storage/AJ3ZS9SZ/2017-CMMR-Breaks.html}
}

@article{lopez-serranoMODELINGDECOMPOSINGLOOPBASED2016,
  title = {{{TOWARDS MODELING AND DECOMPOSING LOOP-BASED ELECTRONIC MUSIC}}},
  author = {{Lopez-Serrano}, Patricio and Dittmar, Christian and Driedger, Jonathan},
  year = {2016},
  journal = {New York City},
  abstract = {Electronic Music (EM) is a popular family of genres which has increasingly received attention as a research subject in the field of MIR. A fundamental structural unit in EM are loops---audio fragments whose length can span several seconds. The devices commonly used to produce EM, such as sequencers and digital audio workstations, impose a musical structure in which loops are repeatedly triggered and overlaid. This particular structure allows new perspectives on well-known MIR tasks. In this paper we first review a prototypical production technique for EM from which we derive a simplified model. We then use our model to illustrate approaches for the following task: given a set of loops that were used to produce a track, decompose the track by finding the points in time at which each loop was activated. To this end, we repurpose established MIR techniques such as fingerprinting and non-negative matrix factor deconvolution.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/L883JIVN/Lopez-Serrano et al. - 2016 - TOWARDS MODELING AND DECOMPOSING LOOP-BASED ELECTRONIC MUSIC.pdf;/home/etiandre/Zotero/storage/VUFH6MCH/2016-ISMIR-EMLoop.html}
}

@misc{magronComplexISNMFPhaseAware2018,
  title = {Complex {{ISNMF}}: A {{Phase-Aware Model}} for {{Monaural Audio Source Separation}}},
  shorttitle = {Complex {{ISNMF}}},
  author = {Magron, Paul and Virtanen, Tuomas},
  year = {2018},
  month = sep,
  number = {arXiv:1802.03156},
  eprint = {1802.03156},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  urldate = {2024-04-22},
  abstract = {This paper introduces a phase-aware probabilistic model for audio source separation. Classical source models in the short-time Fourier transform domain use circularly-symmetric Gaussian or Poisson random variables. This is equivalent to assuming that the phase of each source is uniformly distributed, which is not suitable for exploiting the underlying structure of the phase. Drawing on preliminary works, we introduce here a Bayesian anisotropic Gaussian source model in which the phase is no longer uniform. Such a model permits us to favor a phase value that originates from a signal model through a Markov chain prior structure. The variance of the latent variables are structured with nonnegative matrix factorization (NMF). The resulting model is called complex Itakura-Saito NMF (ISNMF) since it generalizes the ISNMF model to the case of non-isotropic variables. It combines the advantages of ISNMF, which uses a distortion measure adapted to audio and yields a set of estimates which preserve the overall energy of the mixture, and of complex NMF, which enables one to account for some phase constraints. We derive a generalized expectation-maximization algorithm to estimate the model parameters. Experiments conducted on a musical source separation task in a semi-informed setting show that the proposed approach outperforms state-of-the-art phaseaware separation techniques.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/etiandre/Zotero/storage/TJBEGCP5/Magron et Virtanen - 2018 - Complex ISNMF a Phase-Aware Model for Monaural Audio Source Separation.pdf}
}

@misc{MATCHDownloadPage,
  title = {{{MATCH Download Page}}},
  urldate = {2024-03-08},
  howpublished = {http://www.eecs.qmul.ac.uk/{\textasciitilde}simond/match/},
  keywords = {alignment},
  file = {/home/etiandre/Zotero/storage/SMRAE2EB/match.html}
}

@misc{MixedKeySoftware,
  title = {Mixed {{In Key}}: {{Software}} for {{DJs}} and {{Music Producers}}},
  shorttitle = {Mixed {{In Key}}},
  journal = {Mixed In Key},
  urldate = {2024-03-05},
  abstract = {Mixed In Key 10 Software for DJs and Music Producers The world's top DJs and producers use Mixed In Key to help their mixes sound...},
  howpublished = {https://mixedinkey.com/},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/DN6SKGNU/mixedinkey.com.html}
}

@misc{MixesDB,
  title = {{{MixesDB}}},
  journal = {MixesDB - The database for mixes},
  urldate = {2024-03-05},
  howpublished = {//www.mixesdb.com/w/Main\_Page},
  langid = {english},
  keywords = {dataset},
  file = {/home/etiandre/Zotero/storage/RRBR9KQT/Main_Page.html}
}

@misc{MixxxSrcControllers,
  title = {Mixxx/Src/Controllers at Main {$\cdot$} Mixxxdj/Mixxx},
  urldate = {2024-03-13},
  howpublished = {https://github.com/mixxxdj/mixxx/tree/main/src/controllers}
}

@misc{muellerMeinardmuellerSynctoolbox2024,
  title = {Meinardmueller/Synctoolbox},
  author = {Mueller, Meinard},
  year = {2024},
  month = mar,
  urldate = {2024-03-13},
  abstract = {Sync Toolbox - Python package with reference implementations for efficient, robust, and accurate music synchronization based on dynamic time warping (DTW)},
  keywords = {chromagram,dtw,music,onset,python,synchronization}
}

@misc{NetaudioNetlabelResources,
  title = {Netaudio \& {{Netlabel Resources}}},
  journal = {Netlabels.org},
  urldate = {2024-03-08},
  abstract = {Netlabels.org is your resource for netlabel and netaudio culture. Find free music in our archive and find out more about netlabel history.},
  howpublished = {https://netlabels.org/resources/},
  langid = {english},
  keywords = {dataset},
  file = {/home/etiandre/Zotero/storage/LT8LGX37/resources.html}
}

@misc{ohokofficialIndustryStandardDJ2022,
  type = {Reddit {{Post}}},
  title = {Industry Standard {{DJ}} Equipment?},
  author = {{ohokofficial}},
  year = {2022},
  month = jul,
  journal = {r/edmproduction},
  urldate = {2024-03-13}
}

@article{patePredictingDecayTime2014,
  title = {Predicting the Decay Time of Solid Body Electric Guitar Tones},
  author = {Pat{\'e}, Arthur and Le Carrou, Jean-Lo{\"i}c and Fabre, Beno{\^i}t},
  year = {2014},
  month = may,
  journal = {The Journal of the Acoustical Society of America},
  volume = {135},
  number = {5},
  pages = {3045--3055},
  issn = {0001-4966, 1520-8524},
  doi = {10.1121/1.4871360},
  urldate = {2024-01-29},
  abstract = {Although it can be transformed by various electronic devices, the sound of the solid body electric guitar originates from, and is strongly linked with, the string vibration. The coupling of the string with the guitar alters its vibration and can lead to decay time inhomogeneities. This paper implements and justifies a framework for the study of decay times of electric guitar tones. Two damping mechanisms are theoretically and experimentally identified: the string intrinsic damping and the damping due to mechanical coupling with the neck of the guitar. The electromagnetic pickup is shown to not provide any additional damping to the string. The pickup is also shown to be far more sensitive to the out-of-plane polarization of the string. Finally, an accurate prediction of the decay time of electric guitar tones is made possible, whose only requirements are the knowledge of the isolated string dampings and the out-of-plane conductance at the neck of the guitar. This prediction can be of great help for instrument makers and manufacturers.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/BPKM23DK/Paté et al. - 2014 - Predicting the decay time of solid body electric g.pdf}
}

@article{peetersMasterATIAMEstimationa,
  title = {{Master 2 ATIAM   Estimation de fr{\'e}quences fondamentales multiples}},
  author = {Peeters, Geoffroy},
  langid = {french},
  file = {/home/etiandre/Zotero/storage/PFWIVGUV/PeetersNote - Master 2 ATIAM   Estimation de fréquences fondamentales multiples.pdf}
}

@article{raissiPhysicsinformedNeuralNetworks2019,
  title = {Physics-Informed Neural Networks: {{A}} Deep Learning Framework for Solving Forward and Inverse Problems Involving Nonlinear Partial Differential Equations},
  shorttitle = {Physics-Informed Neural Networks},
  author = {Raissi, M. and Perdikaris, P. and Karniadakis, G.E.},
  year = {2019},
  month = feb,
  journal = {Journal of Computational Physics},
  volume = {378},
  pages = {686--707},
  issn = {00219991},
  doi = {10.1016/j.jcp.2018.10.045},
  urldate = {2024-01-25},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/PQ37R8V8/Raissi et al. - 2019 - Physics-informed neural networks A deep learning .pdf}
}

@article{ramonaAutomaticAlignmentAudio2011,
  title = {Automatic Alignment of Audio Occurrences: {{Application}} to the Verification and Synchronization of Audio Fingerprinting Annotation},
  shorttitle = {Automatic Alignment of Audio Occurrences},
  author = {Ramona, Mathieu and Peeters, Geoffroy},
  year = {2011},
  month = jan,
  pages = {429--436},
  abstract = {We propose here an original method for the automatic alignment of temporally distorted occurrences of audio items. The method is based on a so-called item-restricted fingerprinting process and a segment detection scheme. The high-precision estimation of the temporal distortions allows to compensate these alterations and obtain a perfect synchronization between the original item and the altered occurrence. Among the applications of this process, we focus on the verification and the alignment of audio fingerprinting annotations. Perceptual evaluation confirms the efficiency of the method in detecting wrong annotations, and confirms the high precision of the synchronization on the occurrences.},
  file = {/home/etiandre/Zotero/storage/HLJCVISP/Ramona et Peeters - 2011 - Automatic alignment of audio occurrences Application to the verification and synchronization of aud.pdf}
}

@article{ramonaSimpleEfficientFader2011,
  title = {A Simple and Efficient Fader Estimator for Broadcast Radio Unmixing},
  author = {Ramona, Mathieu and Richard, Ga{\"e}l},
  year = {2011},
  month = sep,
  journal = {Proc. Digital Audio Effects (DAFx)},
  pages = {265--268},
  abstract = {This paper presents a framework for the estimation of the faders gain of a mixing console, in the context of broadcast radio production. The retrieval of the console state is generally only possible through a human-machine interface and does not permit the automatic processing of such information. A simple algorithm is provided to estimate the faders position from the different inputs and the output signal of the console. This method also allows the extraction of an additional unknown input, present in the mix output. An exhaustive study on the optimal parameter setting is then detailed, that shows good results on the estimation.},
  langid = {english},
  keywords = {unread},
  file = {/home/etiandre/stage/biblio/RamonaRichard -dafx2011- unmixing bibA12.pdf}
}

@misc{riouPESTOPitchEstimation2023,
  title = {{{PESTO}}: {{Pitch Estimation}} with {{Self-supervised Transposition-equivariant Objective}}},
  shorttitle = {{{PESTO}}},
  author = {Riou, Alain and Lattner, Stefan and Hadjeres, Ga{\"e}tan and Peeters, Geoffroy},
  year = {2023},
  month = sep,
  number = {arXiv:2309.02265},
  eprint = {2309.02265},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  urldate = {2024-03-11},
  abstract = {In this paper, we address the problem of pitch estimation using Self Supervised Learning (SSL). The SSL paradigm we use is equivariance to pitch transposition, which enables our model to accurately perform pitch estimation on monophonic audio after being trained only on a small unlabeled dataset. We use a lightweight ({$<$} 30k parameters) Siamese neural network that takes as inputs two different pitch-shifted versions of the same audio represented by its Constant-Q Transform. To prevent the model from collapsing in an encoder-only setting, we propose a novel class-based transposition-equivariant objective which captures pitch information. Furthermore, we design the architecture of our network to be transposition-preserving by introducing learnable Toeplitz matrices.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {/home/etiandre/Zotero/storage/PP648N9U/Riou et al. - 2023 - PESTO Pitch Estimation with Self-supervised Transposition-equivariant Objective.pdf}
}

@article{rochaSegmentationTimbreRhythmsimilarity2013,
  title = {Segmentation and Timbre- and Rhythm-Similarity in {{Electronic Dance Music}}},
  author = {Rocha, Bruno and Bogaards, Niels and Honingh, Aline},
  year = {2013},
  abstract = {This report describes the digital humanities project on music similarity. The project is a collaboration between the University of Amsterdam and audio software company Elephantcandy. The project's aim was to investigate timbre and rhythm similarity and to develop an application that finds similar segments of music. In this report three models are described, one for structural segmentation, one for timbre similarity, and one for rhythm similarity of electronic dance music (EDM). The segmentation algorithm performs well on an EDM dataset as well as on a standard MIREX dataset. The timbre similarity algorithm has been tested in a pilot study and preliminary results are presented. Issues related to segmentation and similarity are discussed.},
  langid = {english},
  keywords = {unread},
  file = {/home/etiandre/stage/biblio/RochaBogaardsHoningh -2013- Segmentation timbre rhythm similarity Electronic Dance Music  PP-2013-10.text.pdf}
}

@incollection{scarfeLongRangeSelfsimilarityApproach2013,
  title = {A {{Long-Range Self-similarity Approach}} to {{Segmenting DJ Mixed Music Streams}}},
  booktitle = {Artificial {{Intelligence Applications}} and {{Innovations}}},
  author = {Scarfe, Tim and Koolen, Wouter M. and Kalnishkan, Yuri},
  editor = {Papadopoulos, Harris and Andreou, Andreas S. and Iliadis, Lazaros and Maglogiannis, Ilias},
  year = {2013},
  volume = {412},
  pages = {235--244},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-41142-7_24},
  urldate = {2024-03-25},
  abstract = {In this paper we describe an unsupervised, deterministic algorithm for segmenting DJ-mixed Electronic Dance Music (EDM) streams (for example; podcasts, radio shows, live events) into their respective tracks. We attempt to reconstruct boundaries as close as possible to what a human domain expert would engender. The goal of DJ-mixing is to render track boundaries effectively invisible from the standpoint of human perception which makes the problem difficult.},
  isbn = {978-3-642-41141-0 978-3-642-41142-7},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/JXZL69QX/Scarfe et al. - 2013 - A Long-Range Self-similarity Approach to Segmenting DJ Mixed Music Streams.pdf}
}

@inproceedings{scarfeLongRangeSelfsimilarityApproach2013a,
  title = {A {{Long-Range Self-similarity Approach}} to {{Segmenting DJ Mixed Music Streams}}},
  booktitle = {9th {{Artificial Intelligence Applications}} and {{Innovations}} ({{AIAI}})},
  author = {Scarfe, Tim and Koolen, Wouter M. and Kalnishkan, Yuri},
  year = {2013},
  month = sep,
  volume = {AICT-412},
  pages = {235},
  publisher = {Springer},
  doi = {10.1007/978-3-642-41142-7_24},
  urldate = {2024-03-27},
  abstract = {In this paper we describe an unsupervised, deterministic algorithm for segmenting DJ-mixed Electronic Dance Music streams (for example; podcasts, radio shows, live events) into their respective tracks. We attempt to reconstruct boundaries as close as possible to what a human domain expert would engender. The goal of DJ-mixing is to render track boundaries effectively invisible from the standpoint of human perception which makes the problem difficult.We use Dynamic Programming (DP) to optimally segment a cost matrix derived from a similarity matrix. The similarity matrix is based on the cosines of a time series of kernel-transformed Fourier based features designed with this domain in mind. Our method is applied to EDM streams. Its formulation incorporates long-term self similarity as a first class concept combined with DP and it is qualitatively assessed on a large corpus of long streams that have been hand labelled by a domain expert.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/8E3GVW56/Scarfe et al. - 2013 - A Long-Range Self-similarity Approach to Segmenting DJ Mixed Music Streams.pdf}
}

@misc{SceneOrgFile,
  title = {Scene.Org File Archive :: Browsing /Music/Groups/},
  urldate = {2024-03-08},
  howpublished = {https://files.scene.org/browse/music/groups/},
  keywords = {dataset},
  file = {/home/etiandre/Zotero/storage/AEKNMCD6/groups.html}
}

@article{schremmerWAVELETSREALTIMEDIGITAL,
  title = {{{WAVELETS IN REAL-TIME DIGITAL AUDIO PROCESSING}}: {{A SOFTWARE FOR UNDERSTANDING WAVELETS IN APPLIED COMPUTER SCIENCE}}},
  author = {Schremmer, Claudia and Haenselmann, Thomas and B{\"o}mers, Florian},
  abstract = {Today increasing hardware performance and the development of faster and more efficient algorithms allow the implementation of signal processors even on PCs. This article discusses real-time processing of digital audio with wavelets and multiresolution analysis. We present a tool that allows the real-time application and modification of wavelet filters. Thus, it allows the user to directly hear the effects of parameter settings. Simultaneously, it visualizes the time domain of the audio signal as well as the timescale domain of the multiresolution.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/QD2ZLICR/Schremmer et al. - WAVELETS IN REAL-TIME DIGITAL AUDIO PROCESSING A SOFTWARE FOR UNDERSTANDING WAVELETS IN APPLIED COM.pdf}
}

@article{schwarzExtractionGroundTruth,
  title = {Towards {{Extraction}} of {{Ground Truth Data}} from {{DJ Mixes}}},
  author = {Schwarz, Diemo and Fourer, Dominique},
  journal = {ISMIR},
  pages = {2017},
  abstract = {DJ techniques are an important part of popular music culture but that are so far not very well researched because of the lack of annotated databases of DJ mixes. We first offer an overview of the necessary components to annotate recorded mixes automatically, which are: fingerprinting to obtain the tracklist, alignment to determine where in the mix each track starts and what tempo changes are applied to acheive beat-synchronicity, unmixing to estimate the fade curves for volume, bass and treble, and content and metadata analysis to derive the genre and social tags attached to the music to inform about the choices a DJ makes when creating a mix. Most of these components have been addressed by recent MIR research, except the alignment part for which we will give a first attempt using multi-scale correlation and dynamic time warping.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/EBD37T33/Schwarz et Fourer - Towards Extraction of Ground Truth Data from DJ Mixes.pdf}
}

@article{schwarzHeuristicAlgorithmDJ2018,
  title = {A {{Heuristic Algorithm}} for {{DJ Cue Point Estimation}}},
  author = {Schwarz, Diemo and Schindler, Daniel Artur and Spadavecchia, Severino},
  year = {2018},
  abstract = {This article treats an aspect in a larger research agenda to understand DJ practices, which are an important part of popular music culture: We present a heuristic algorithm that estimates cue points where tracks should cross-fade in a DJ mix. We deduced statistics and heuristics from a list of rules provided by human experts, and from a database of example tracks with given cue regions. We then created an algorithm for cue-point estimation based on rich automatic annotations by state of the art MIR methods, such as music structure segmentation and beat tracking. The results were evaluated quantitatively on the example database and qualitatively by experts.},
  langid = {english},
  file = {/home/etiandre/stage/biblio/schwarz-smc2018-cuepoints.pdf}
}

@incollection{schwarzMethodsDatasetsDJMix2021,
  title = {Methods and {{Datasets}} for {{DJ-Mix Reverse Engineering}}},
  booktitle = {Perception, {{Representations}}, {{Image}}, {{Sound}}, {{Music}}},
  author = {Schwarz, Diemo and Fourer, Dominique},
  editor = {{Kronland-Martinet}, Richard and Ystad, S{\o}lvi and Aramaki, Mitsuko},
  year = {2021},
  volume = {12631},
  pages = {31--47},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-70210-6_2},
  urldate = {2024-03-05},
  abstract = {DJ techniques are an important part of popular music culture. However, they are also not sufficiently investigated by researchers due to the lack of annotated datasets of DJ mixes. Thus, this paper aims at filling this gap by introducing novel methods to automatically deconstruct and annotate recorded mixes for which the constituent tracks are known. A rough alignment first estimates where in the mix each track starts, and which time-stretching factor was applied. Second, a sampleprecise alignment is applied to determine the exact offset of each track in the mix. Third, we propose a new method to estimate the cue points and the fade curves which operates in the time-frequency domain to increase its robustness to interference with other tracks. The proposed methods are finally evaluated on our new publicly available DJ-mix dataset UnmixDB. This dataset contains automatically generated beat-synchronous mixes based on freely available music tracks, and the ground truth about the placement, transformations and effects of tracks in a mix.},
  isbn = {978-3-030-70209-0 978-3-030-70210-6},
  langid = {english},
  keywords = {alignment,dataset,extraction,fader},
  file = {/home/etiandre/stage/biblio/schwarz-fourer-lncs2020-unmixing.pdf}
}

@article{schwarzUnmixDBDatasetDJMix2018,
  title = {{{UnmixDB}}:  {{A Dataset}} for {{DJ-Mix Information Retrieval}}},
  shorttitle = {{{UnmixDB}}},
  author = {Schwarz, Diemo and Fourer, Dominique},
  year = {2018},
  month = sep,
  journal = {ISMIR},
  urldate = {2024-03-05},
  abstract = {DJ techniques are an important part of popular music culture but are also not sufficiently investigated by researchers due to the lack of annotated datasets of DJ mixes. This paper aims at filling this gap by introducing a publicly available DJ-mix dataset. It contains automatically generated beat-synchronous mixes based on freely available music tracks, and the ground truth about the placement of tracks in a mix. Each mix is generated in several variants with different effects and time-stretching methods applied. Possible applications are to test novel methods for track identification in mixes, or for automatic annotation and deconstruction of recorded mixes for which the constituent tracks are known.},
  langid = {english},
  keywords = {dataset,DJ mix,MIR,music audio,music information retrieval,songs,tracks},
  file = {/home/etiandre/stage/biblio/schwarz-fourer-ismir2018late-breaking-unmixdb.pdf;/home/etiandre/Zotero/storage/GB7DFLZP/Schwarz et Fourer - 2018 - UnmixDB  A Dataset for DJ-Mix Information Retrieval.pdf}
}

@article{sitarcikWarpSTRDeterminingTandem2023,
  title = {{{WarpSTR}}: Determining Tandem Repeat Lengths Using Raw Nanopore Signals},
  shorttitle = {{{WarpSTR}}},
  author = {Sitar{\v c}{\'i}k, Jozef and Vina{\v r}, Tom{\'a}{\v s} and Brejov{\'a}, Bro{\v n}a and Krampl, Werner and Budi{\v s}, Jaroslav and Radv{\'a}nszky, J{\'a}n and Luck{\'a}, M{\'a}ria},
  year = {2023},
  month = jun,
  journal = {Bioinformatics (Oxford, England)},
  volume = {39},
  number = {6},
  pages = {btad388},
  issn = {1367-4811},
  doi = {10.1093/bioinformatics/btad388},
  abstract = {MOTIVATION: Short tandem repeats (STRs) are regions of a genome containing many consecutive copies of the same short motif, possibly with small variations. Analysis of STRs has many clinical uses but is limited by technology mainly due to STRs surpassing the used read length. Nanopore sequencing, as one of long-read sequencing technologies, produces very long reads, thus offering more possibilities to study and analyze STRs. Basecalling of nanopore reads is however particularly unreliable in repeating regions, and therefore direct analysis from raw nanopore data is required. RESULTS: Here, we present WarpSTR, a novel method for characterizing both simple and complex tandem repeats directly from raw nanopore signals using a finite-state automaton and a search algorithm analogous to dynamic time warping. By applying this approach to determine the lengths of 241 STRs, we demonstrate that our approach decreases the mean absolute error of the STR length estimate compared to basecalling and STRique. AVAILABILITY AND IMPLEMENTATION: WarpSTR is freely available at https://github.com/fmfi-compbio/warpstr.},
  langid = {english},
  pmcid = {PMC10307940},
  pmid = {37326967},
  keywords = {Algorithms,Genome,High-Throughput Nucleotide Sequencing,Microsatellite Repeats,Nanopores,Sequence Analysis DNA},
  file = {/home/etiandre/Zotero/storage/2RRA6IUH/Sitarčík et al. - 2023 - WarpSTR determining tandem repeat lengths using raw nanopore signals.pdf}
}

@inproceedings{sixDiscStitchAudiotoaudioAlignment2022,
  title = {{{DiscStitch}} : Towards Audio-to-Audio Alignment with Robustness to Playback Speed Variabilities},
  shorttitle = {{{DiscStitch}}},
  booktitle = {Extended {{Abstracts}} for the {{Late-Breaking Demo Session}} of the 23rd {{International Society}} for {{Music Information Retrieval Conference}}},
  author = {Six, Joren},
  year = {2022},
  publisher = {ISMIR},
  urldate = {2024-03-06},
  abstract = {Before magnetic tape recording was common, acetate discs were the main audio storage medium for radio broadcasters. Acetate discs only had a capacity to record about ten minutes. Longer material was recorded on overlapping discs using (at least) two recorders. Unfortunately, the recorders used were not reliable in terms of recording speed, resulting in audio of variable speed. To make digitized audio originating from acetate discs fit for reuse, (1) overlapping parts need to be identified, (2) a precise alignment needs to be found and (3) a mixing point suggested. All three steps are challenging due to the audio speed variabilities. This paper introduces the ideas behind DiscStitch: which aims to reassemble audio from overlapping parts, even if variable speed is present. The main contribution is a fast and precise audio alignment strategy based on spectral peaks. The method is evaluated on a synthetic data set.},
  copyright = {Creative Commons Attribution 4.0 International Public License (CC-BY 4.0)},
  langid = {english},
  keywords = {unread},
  file = {/home/etiandre/Zotero/storage/4NE5J88T/Six - 2022 - DiscStitch  towards audio-to-audio alignment with robustness to playback speed variabilities.pdf}
}

@inproceedings{sixDiscStitchAudiotoaudioAlignment2022a,
  title = {{{DiscStitch}} : Towards Audio-to-Audio Alignment with Robustness to Playback Speed Variabilities},
  shorttitle = {{{DiscStitch}}},
  booktitle = {Extended {{Abstracts}} for the {{Late-Breaking Demo Session}} of the 23rd {{International Society}} for {{Music Information Retrieval Conference}}},
  author = {Six, Joren},
  year = {2022},
  publisher = {ISMIR},
  urldate = {2024-03-11},
  abstract = {Before magnetic tape recording was common, acetate discs were the main audio storage medium for radio broadcasters. Acetate discs only had a capacity to record about ten minutes. Longer material was recorded on overlapping discs using (at least) two recorders. Unfortunately, the recorders used were not reliable in terms of recording speed, resulting in audio of variable speed. To make digitized audio originating from acetate discs fit for reuse, (1) overlapping parts need to be identified, (2) a precise alignment needs to be found and (3) a mixing point suggested. All three steps are challenging due to the audio speed variabilities. This paper introduces the ideas behind DiscStitch: which aims to reassemble audio from overlapping parts, even if variable speed is present. The main contribution is a fast and precise audio alignment strategy based on spectral peaks. The method is evaluated on a synthetic data set.},
  copyright = {Creative Commons Attribution 4.0 International Public License (CC-BY 4.0)},
  langid = {english},
  keywords = {alignment},
  file = {/home/etiandre/Zotero/storage/VNNIYJ46/Six - 2022 - DiscStitch  towards audio-to-audio alignment with robustness to playback speed variabilities.pdf}
}

@article{sixOlafLightweightPortable2023,
  title = {Olaf : A Lightweight, Portable Audio Search System},
  shorttitle = {Olaf},
  author = {Six, Joren},
  year = {2023},
  journal = {JOURNAL OF OPEN SOURCE SOFTWARE},
  volume = {8},
  number = {87},
  issn = {2475-9066},
  doi = {10.21105/joss.05459},
  urldate = {2024-03-06},
  abstract = {Olaf stands for Overly Lightweight Acoustic Fingerprinting and solves the problem of finding short audio fragments in large digital audio archives. The content-based audio search algorithm implemented in Olaf can identify a short audio query in a large database of thousands of hours of audio using an acoustic fingerprinting technique.},
  copyright = {Creative Commons Attribution 4.0 International Public License (CC-BY 4.0)},
  langid = {english},
  keywords = {unread},
  file = {/home/etiandre/Zotero/storage/25HL2DTY/Six - 2023 - Olaf  a lightweight, portable audio search system.pdf}
}

@inproceedings{sixPanakoScalableAcoustic2014,
  title = {Panako: A Scalable Acoustic Fingerprinting System Handling Time-Scale and Pitch Modification},
  shorttitle = {Panako},
  booktitle = {International {{Society}} for {{Music Information Retrieval}}, {{Proceedings}}},
  author = {Six, Joren and Leman, Marc},
  year = {2014},
  volume = {Proceedings of the 15th Conference of the International Society for Music Information Retrieval (ISMIR 2014)},
  urldate = {2024-03-06},
  abstract = {In this paper a scalable granular acoustic fingerprinting system robust against time and pitch scale modification is presented. The aim of acoustic fingerprinting is to identify identical, or recognize similar, audio fragments in a large set using condensed representations of audio signals, i.e. fingerprints. A robust fingerprinting system generates similar fingerprints for perceptually similar audio signals. The new system, presented here, handles a variety of distortions well. It is designed to be robust against pitch shifting, time stretching and tempo changes, while remaining scalable. After a query, the system returns the start time in the reference audio, and the amount of pitch shift and tempo change that has been applied. The design of the system that offers this unique combination of features is the main contribution of this research. The fingerprint itself consists of a combination of key points in a Constant-Q spectrogram. The system is evaluated on commodity hardware using a freely available reference database with fingerprints of over 30.000 songs. The results show that the system responds quickly and reliably on queries, while handling time and pitch scale modifications of up to ten percent.},
  copyright = {No license (in copyright)},
  langid = {english},
  keywords = {unread},
  file = {/home/etiandre/Zotero/storage/ITE4JY9Q/Six et Leman - 2014 - Panako a scalable acoustic fingerprinting system handling time-scale and pitch modification.pdf}
}

@article{sixPanakoScalableAudio2022,
  title = {Panako : A Scalable Audio Search System},
  shorttitle = {Panako},
  author = {Six, Joren},
  year = {2022},
  journal = {JOURNAL OF OPEN SOURCE SOFTWARE},
  volume = {7},
  number = {78},
  issn = {2475-9066},
  doi = {10.21105/joss.04554},
  urldate = {2024-03-06},
  abstract = {Panako solves the problem of finding short audio fragments in large digital audio archives. The content based audio search algorithm implemented in Panako is able to identify a short audio query in large database of thousands of hours of audio using an acoustic fingerprinting technique.  The dataflow in Panako closely resembles the flow depicted in fig 1. Audio is transformed to features which are grouped in recognizable fingerprints. The fingerprints are compared with a database of reference fingerprints. If a match is found, it is reported or, in case of a true negative, the system reports that the audio is not present in the database. The properties of the acoustic fingerprinting system mainly depend on the selection of features, the information captured by the fingerprints and the performance of the matching step. There are three algorithms implemented in Panako. The first is based on pitch class histograms. The other two are based on peaks in a spectral representation: a baseline algorithm and the main Panako algorithm. The main algorithm in Panako finds matches between a short query fragment and the audio in the database even if the query is time-stretched, pitch-shifted, slowed down or sped up while maintaining other desirable features such as scalability, robustness and reliability. This is important since changes in replay speed do occur commonly, they are either introduced by accident during an analog to digital conversion or are introduced deliberately.  Accidental audio speed changes are often the result of varying or uncalibrated recording or playback speeds of analogue physical media such as wax cylinders, wire recordings, magnetic tapes or gramophone records. To identify duplicates in a digitized archive, a music search algorithm should be robust against replay speed. Deliberate speed manipulations are sometimes introduced during radio broadcasts: occasionally songs are played a bit faster to fit into a time-slot. During a DJ-set speed changes are almost always present. To correctly identify audio in these cases as well, a music search algorithm must be robust against pitch shifting, time stretching and speed changes.},
  copyright = {Creative Commons Attribution 4.0 International Public License (CC-BY 4.0)},
  langid = {english},
  keywords = {unread},
  file = {/home/etiandre/Zotero/storage/VAZ34IJZ/Six - 2022 - Panako  a scalable audio search system.pdf}
}

@inproceedings{sixPanakoUpdatesAcoustic2021,
  title = {Panako 2.0 : Updates for an Acoustic Fingerprinting System},
  shorttitle = {Panako 2.0},
  booktitle = {{{ISMIR}} 2021 {{Late-breaking}} Demo Contributions},
  author = {Six, Joren},
  year = {2021},
  urldate = {2024-03-06},
  abstract = {This work presents updates to Panako, an acoustic fingerprinting system that was introduced at ISMIR 2014. The notable feature of Panako is that it matches queries even after a speedup, time-stretch or pitch-shift. It is freely available and has no problems indexing and querying 100k sea shanties. The updates presented here improve query performance significantly and allow a wider range of time-stretch, pitch-shift and speed-up factors: e.g. the top 1 true positive rate for 20s query that were sped up by 10 percent increased from 18\% to 83\% from the 2014 version of Panako to the new version. The aim of this short write-up is to reintroduce Panako, evaluate the improvements and highlight two techniques with wider applicability. The first of the two techniques is the use of a constant-Q non-stationary Gabor transform: a fast, reversible, fine-grained spectral transform which can be used as a front-end for many MIR tasks. The second is how near-exact hashing is used in combination with a persistent B-Tree to allow some margin of error while maintaining reasonable query speeds.},
  copyright = {Creative Commons Attribution 4.0 International Public License (CC-BY 4.0)},
  langid = {english},
  keywords = {unread},
  file = {/home/etiandre/Zotero/storage/HAN3ZJEH/Six - 2021 - Panako 2.0  updates for an acoustic fingerprinting system.pdf}
}

@incollection{smaragdisNonnegativeMatrixFactor2004,
  title = {Non-Negative {{Matrix Factor Deconvolution}}; {{Extraction}} of {{Multiple Sound Sources}} from {{Monophonic Inputs}}},
  booktitle = {Independent {{Component Analysis}} and {{Blind Signal Separation}}},
  author = {Smaragdis, Paris},
  editor = {Puntonet, Carlos G. and Prieto, Alberto},
  year = {2004},
  volume = {3195},
  pages = {494--499},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-30110-3_63},
  urldate = {2024-04-10},
  abstract = {In this paper we present an extension to the Non-Negative Matrix Factorization algorithm which is capable of identifying components with temporal structure. We demonstrate the use of this algorithm in the magnitude spectrum domain, where we employ it to perform extraction of multiple sound objects from a single channel auditory scene.},
  isbn = {978-3-540-23056-4 978-3-540-30110-3},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/46MYSKGP/Smaragdis - 2004 - Non-negative Matrix Factor Deconvolution; Extraction of Multiple Sound Sources from Monophonic Input.pdf}
}

@incollection{smaragdisSupervisedSemisupervisedSeparation2007,
  title = {Supervised and {{Semi-supervised Separation}} of {{Sounds}} from {{Single-Channel Mixtures}}},
  booktitle = {Independent {{Component Analysis}} and {{Signal Separation}}},
  author = {Smaragdis, Paris and Raj, Bhiksha and Shashanka, Madhusudana},
  editor = {Davies, Mike E. and James, Christopher J. and Abdallah, Samer A. and Plumbley, Mark D},
  year = {2007},
  volume = {4666},
  pages = {414--421},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-74494-8_52},
  urldate = {2024-04-19},
  abstract = {In this paper we describe a methodology for model-based single channel separation of sounds. We present a sparse latent variable model that can learn sounds based on their distribution of time/frequency energy. This model can then be used to extract known types of sounds from mixtures in two scenarios. One being the case where all sound types in the mixture are known, and the other being being the case where only the target or the interference models are known. The model we propose has close ties to non-negative decompositions and latent variable models commonly used for semantic analysis.},
  isbn = {978-3-540-74493-1 978-3-540-74494-8},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/IHBQHLSL/Smaragdis et al. - 2007 - Supervised and Semi-supervised Separation of Sounds from Single-Channel Mixtures.pdf}
}

@inproceedings{smithNonnegativeTensorFactorization2018,
  title = {Nonnegative {{Tensor Factorization}} for {{Source Separation}} of {{Loops}} in {{Audio}}},
  booktitle = {2018 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Smith, Jordan B. L. and Goto, Masataka},
  year = {2018},
  month = apr,
  pages = {171--175},
  publisher = {IEEE},
  address = {Calgary, AB},
  doi = {10.1109/ICASSP.2018.8461876},
  urldate = {2024-03-05},
  abstract = {The prevalence of exact repetition in loop-based music makes it an opportune target for source separation. Nonnegative factorization approaches have been used to model the repetition of looped content, and kernel additive modeling has leveraged periodicity within a piece to separate looped background elements. We propose a novel method of leveraging periodicity in a factorization model: we treat the two-dimensional spectrogram as a three-dimensional tensor, and use nonnegative tensor factorization to estimate the component spectral templates, rhythms and loop recurrences in a single step. Testing our method on synthesized loop-based examples, we find that our algorithm mostly exceeds the performance of competing methods, with a reduction in execution cost. We discuss limitations of the algorithm as we demonstrate its potential to analyze larger and more complex songs.},
  isbn = {978-1-5386-4658-8},
  langid = {english},
  file = {/home/etiandre/stage/biblio/SmithGoto -ICASSP2018- nonnegative tensor factorization loops.pdf}
}

@article{smithUNMIXERINTERFACEEXTRACTING2019,
  title = {{{UNMIXER}}: {{AN INTERFACE FOR EXTRACTING AND REMIXING LOOPS}}},
  author = {Smith, Jordan B L and Kawasaki, Yuta and Goto, Masataka},
  year = {2019},
  abstract = {To create their art, remix artists would like to have segmented stem tracks at their disposal; that is, isolated instances of the loops and sounds that the original composer used to create a track. We present Unmixer, a web service that will analyze and extract loops from any audio uploaded by a user. The loops are presented in an interface that allows users to immediately remix the loops; if users upload multiple tracks, they can easily create mashups with the loops, which are automatically matched in tempo. To analyze the audio, we use a recently-proposed method of source separation based on the nonnegative Tucker decomposition of the spectrum. To reduce interference among the extracted loops, we propose an extra factorization step with a sparseness constraint and demonstrate that it improves the source separation result. We also propose a method for selecting the best instances of the extracted loops and demonstrate its effectiveness in an evaluation. Both of these improvements are incorporated into the backend of the interface. Finally, we discuss the feedback collected in a set of user evaluations.},
  langid = {english},
  file = {/home/etiandre/stage/biblio/Smith -ismir2019- unmixer_interface.pdf}
}

@article{sonnleitnerLandmarkBasedAudioFingerprinting2016,
  title = {Landmark-{{Based Audio Fingerprinting}} for {{DJ Mix Monitoring}}},
  author = {Sonnleitner, Reinhard and Arzt, Andreas and Widmer, Gerhard},
  year = {2016},
  journal = {ISMIR},
  abstract = {Recently, the media monitoring industry shows increased interest in applying automated audio identification systems for revenue distribution of DJ performances played in discotheques. DJ mixes incorporate a wide variety of signal modifications, e.g. pitch shifting, tempo modifications, cross-fading and beat-matching. These signal modifications are expected to be more severe than what is usually encountered in the monitoring of radio and TV broadcasts. The monitoring of DJ mixes presents a hard challenge for automated music identification systems, which need to be robust to various signal modifications while maintaining a high level of specificity to avoid false revenue assignment. In this work we assess the fitness of three landmark-based audio fingerprinting systems with different properties on real-world data -- DJ mixes that were performed in discotheques. To enable the research community to evaluate systems on DJ mixes, we also create and publish a freely available, creative-commons licensed dataset of DJ mixes along with their reference tracks and song-border annotations. Experiments on these datasets reveal that a recent quad-based method achieves considerably higher performance on this task than the other methods.},
  langid = {english},
  keywords = {identification},
  file = {/home/etiandre/stage/biblio/SonnleitnerArztWidmer -ismir2016- Landmark-based Audio Fingerprinting for DJ Mix Monitoring 187.pdf.pdf}
}

@article{suematsuTimeSeriesAlignment,
  title = {Time {{Series Alignment}} with {{Gaussian Processes}}},
  author = {Suematsu, Nobuo and Hayashi, Akira},
  abstract = {We propose a nonparametric Bayesian approach to time series alignment. Time series alignment is a technique often required when we analyze a set of time series in which there exists a typical structural pattern common to all the time series. Such a set of time series is typically obtained by repeated measurements of a biological, chemical or physical process. In time series alignment, we are required to estimate a common shape function, which describes a common structural patter shared among a set of time series, and time transformation functions, each of which represents time shifts involved in individual time series. In this paper, we introduce a generative model for time series data in which the common shape function and the time transformation functions are modeled nonparametrically using Gaussian processes and we develop an effective Markov Chain Monte Carlo algorithm, which realizes a nonparametric Bayesian approach to time series alignment. The effectiveness of our method is demonstrated in an experiment with synthetic data and an experiment with real time series data is also presented.},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/W7PCESRJ/Suematsu et Hayashi - Time Series Alignment with Gaussian Processes.pdf}
}

@inproceedings{sunAlternatingDirectionMethod2014,
  title = {Alternating Direction Method of Multipliers for Non-Negative Matrix Factorization with the Beta-Divergence},
  booktitle = {2014 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Sun, Dennis L. and F{\'e}votte, C{\'e}dric},
  year = {2014},
  month = may,
  pages = {6201--6205},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2014.6854796},
  urldate = {2024-04-19},
  abstract = {Non-negative matrix factorization (NMF) is a popular method for learning interpretable features from non-negative data, such as counts or magnitudes. Different cost functions are used with NMF in different applications. We develop an algorithm, based on the alternating direction method of multipliers, that tackles NMF problems whose cost function is a beta-divergence, a broad class of divergence functions. We derive simple, closed-form updates for the most commonly used beta-divergences. We demonstrate experimentally that this algorithm has faster convergence and yields superior results to state-of-the-art algorithms for this problem.},
  keywords = {alternating direction method of multipliers,beta-divergence,Convergence,Euclidean distance,Noise,non-negative matrix factorization,Optimization,Signal processing algorithms,Speech,Vectors},
  file = {/home/etiandre/Zotero/storage/GGBV6WX9/Sun et Févotte - 2014 - Alternating direction method of multipliers for non-negative matrix factorization with the beta-dive.pdf;/home/etiandre/Zotero/storage/RQRSBUZW/6854796.html}
}

@misc{TestingNewTool,
  title = {{Testing a New Tool for Alignment of Musical Recordings - ProQuest}},
  urldate = {2024-04-15},
  abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
  howpublished = {https://www.proquest.com/openview/c7a651d5917d47ad920c4d8abb839a42/1?pq-origsite=gscholar\&cbl=18750\&diss=y},
  langid = {french},
  file = {/home/etiandre/Zotero/storage/4FQGER7G/Testing a New Tool for Alignment of Musical Recordings - ProQuest.pdf;/home/etiandre/Zotero/storage/D6BQTMHC/1.html}
}

@misc{TrackIdNetYour,
  title = {{{TrackId}}.Net - {{Your}} Source for Tracklists},
  urldate = {2024-03-05},
  howpublished = {https://trackid.net/},
  keywords = {dataset},
  file = {/home/etiandre/Zotero/storage/QSWF6EB7/trackid.net.html}
}

@misc{tralieCtralieLinmdtw2024,
  title = {Ctralie/Linmdtw},
  author = {Tralie, Chris},
  year = {2024},
  month = feb,
  urldate = {2024-03-13},
  abstract = {A linear memory, parallelizable algorithm for Dynamic Time Warping (DTW).  Also contains cython implementations of FastDTW, and Memory-Restricted Multiscale DTW (MrMsDTW), and globally constrained DTW with the Sakoe-Chiba band (so-called "cdtw")},
  copyright = {Apache-2.0}
}

@article{tsaiKNOWNARTISTLIVESONG2016,
  title = {{{KNOWN-ARTIST LIVE SONG ID}}: {{A HASHPRINT APPROACH}}},
  author = {Tsai, {\relax TJ} and Pratzlich, Thomas},
  year = {2016},
  journal = {New York City},
  abstract = {The goal of live song identification is to recognize a song based on a short, noisy cell phone recording of a live performance. We propose a system for known-artist live song identification and provide empirical evidence of its feasibility. The proposed system represents audio as a sequence of hashprints, which are binary fingerprints that are derived from applying a set of spectro-temporal filters to a spectrogram representation. The spectro-temporal filters can be learned in an unsupervised manner on a small amount of data, and can thus tailor its representation to each artist. Matching is performed using a cross-correlation approach with downsampling and rescoring. We evaluate our approach on the Gracenote live song identification benchmark data set, and compare our results to five other baseline systems. Compared to the previous state-of-the-art, the proposed system improves the mean reciprocal rank from .68 to .79, while simultaneously reducing the average runtime per query from 10 seconds down to 0.9 seconds.},
  langid = {english},
  keywords = {identification,unread},
  file = {/home/etiandre/Zotero/storage/YURLK2LN/Tsai et Pratzlich - 2016 - KNOWN-ARTIST LIVE SONG ID A HASHPRINT APPROACH.pdf}
}

@book{valetteMecaniqueCordeVibrante1993,
  title = {{M{\'e}canique de Corde Vibrante}},
  author = {{VALETTE}},
  year = {1993},
  publisher = {Hermes science publications},
  address = {Cachan},
  isbn = {978-2-7462-3499-4},
  langid = {fre},
  annotation = {OCLC: 1267764816},
  file = {/home/etiandre/Zotero/storage/IFXIIFSJ/CuestaValette_MecaniqueDeLaCordeVibrante_Ch4_R.pdf;/home/etiandre/Zotero/storage/NDTW7N7Q/CuestaValette_MecaniqueDeLaCordeVibrante_Ch3_R.pdf}
}

@article{vandeveireMusicInformationRetrieval2021,
  title = {Music {{Information Retrieval Methods}} for {{Analyzing}} and {{Modifying Percussion-Based Audio}}},
  author = {Vande Veire, Len},
  year = {2021},
  month = aug,
  journal = {IIOimport},
  urldate = {2024-03-14},
  langid = {english},
  annotation = {Accepted: 2021-10-31T12:10:33Z},
  file = {/home/etiandre/Zotero/storage/ZNHFDYV6/Vande Veire - 2021 - Music Information Retrieval Methods for Analyzing and Modifying Percussion-Based Audio.pdf;/home/etiandre/Zotero/storage/GDQA2LP3/37308.html}
}

@article{virtanenMonauralSoundSource2007,
  title = {Monaural {{Sound Source Separation}} by {{Nonnegative Matrix Factorization With Temporal Continuity}} and {{Sparseness Criteria}}},
  author = {Virtanen, Tuomas},
  year = {2007},
  month = mar,
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {15},
  number = {3},
  pages = {1066--1074},
  issn = {1558-7924},
  doi = {10.1109/TASL.2006.885253},
  urldate = {2024-04-19},
  abstract = {An unsupervised learning algorithm for the separation of sound sources in one-channel music signals is presented. The algorithm is based on factorizing the magnitude spectrogram of an input signal into a sum of components, each of which has a fixed magnitude spectrum and a time-varying gain. Each sound source, in turn, is modeled as a sum of one or more components. The parameters of the components are estimated by minimizing the reconstruction error between the input spectrogram and the model, while restricting the component spectrograms to be nonnegative and favoring components whose gains are slowly varying and sparse. Temporal continuity is favored by using a cost term which is the sum of squared differences between the gains in adjacent frames, and sparseness is favored by penalizing nonzero gains. The proposed iterative estimation algorithm is initialized with random values, and the gains and the spectra are then alternatively updated using multiplicative update rules until the values converge. Simulation experiments were carried out using generated mixtures of pitched musical instrument samples and drum sounds. The performance of the proposed method was compared with independent subspace analysis and basic nonnegative matrix factorization, which are based on the same linear model. According to these simulations, the proposed method enables a better separation quality than the previous algorithms. Especially, the temporal continuity criterion improved the detection of pitched musical sounds. The sparseness criterion did not produce significant improvements},
  keywords = {Acoustic signal analysis,audio source separation,blind source separation,Costs,Humans,Independent component analysis,Machine learning algorithms,Multiple signal classification,music,Music,nonnegative matrix factorization,Source separation,sparse coding,Sparse matrices,Spectrogram,unsupervised learning,Unsupervised learning},
  file = {/home/etiandre/Zotero/storage/Y3FS7GDR/4100700.html}
}

@article{virtanenMonauralSoundSource2007a,
  title = {Monaural {{Sound Source Separation}} by {{Nonnegative Matrix Factorization With Temporal Continuity}} and {{Sparseness Criteria}}},
  author = {Virtanen, Tuomas},
  year = {2007},
  month = mar,
  journal = {IEEE Transactions on Audio, Speech and Language Processing},
  volume = {15},
  number = {3},
  pages = {1066--1074},
  issn = {1558-7916},
  doi = {10.1109/TASL.2006.885253},
  urldate = {2024-04-19},
  abstract = {An unsupervised learning algorithm for the separation of sound sources in one-channel music signals is presented. The algorithm is based on factorizing the magnitude spectrogram of an input signal into a sum of components, each of which has a fixed magnitude spectrum and a time-varying gain. Each sound source, in turn, is modeled as a sum of one or more components. The parameters of the components are estimated by minimizing the reconstruction error between the input spectrogram and the model, while restricting the component spectrograms to be nonnegative and favoring components whose gains are slowly varying and sparse. Temporal continuity is favored by using a cost term which is the sum of squared differences between the gains in adjacent frames, and sparseness is favored by penalizing nonzero gains. The proposed iterative estimation algorithm is initialized with random values, and the gains and the spectra are then alternatively updated using multiplicative update rules until the values converge. Simulation experiments were carried out using generated mixtures of pitched musical instrument samples and drum sounds. The performance of the proposed method was compared with independent subspace analysis and basic nonnegative matrix factorization, which are based on the same linear model. According to these simulations, the proposed method enables a better separation quality than the previous algorithms. Especially, the temporal continuity criterion improved the detection of pitched musical sounds. The sparseness criterion did not produce significant improvements.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/9FJJ7C7J/Virtanen - 2007 - Monaural Sound Source Separation by Nonnegative Matrix Factorization With Temporal Continuity and Sp.pdf}
}

@misc{VirtualDJVDJPediaHIDimplementation,
  title = {{{VirtualDJ}} - {{VDJPedia}} - {{HIDimplementation}}},
  journal = {VirtualDJ Website},
  urldate = {2024-03-14},
  abstract = {With over 100,000,000 downloads, VirtualDJ packs the most advanced DJ technology. Both perfect to start DJing, and perfect for advanced pro DJs.},
  howpublished = {https://www.virtualdj.com/wiki/HIDImplementation.html},
  langid = {english}
}

@mastersthesis{werthen-brabantsGroundTruthExtraction2018,
  title = {Ground Truth Extraction \& Transition Analysis of {{DJ}} Mixes},
  author = {{Werthen-Brabants}, Lorin and De Bie, Tijl and Simeone, Paolo},
  year = {2018},
  month = aug,
  school = {Ghent University, Ghent, Belgium},
  file = {/home/etiandre/stage/biblio/Werthen-master2018-RUG01-002494859_2018_0001_AC.pdf}
}

@article{woodhouseChoosingStringsPlucked2019,
  title = {Choosing {{Strings}} for {{Plucked Musical Instruments}}},
  author = {Woodhouse, Jim and {Lynch-Aird}, Nicolas},
  year = {2019},
  month = may,
  journal = {Acta Acustica united with Acustica},
  volume = {105},
  number = {3},
  pages = {516--529},
  issn = {1610-1928},
  doi = {10.3813/AAA.919333},
  urldate = {2024-01-29},
  langid = {english},
  file = {/home/etiandre/Zotero/storage/HMG8MTNB/Woodhouse&LynchAird_2019.pdf}
}

@article{yadatiDetectingDropsElectronic2014,
  title = {Detecting {{Drops}} in {{Electronic Dance Music}}: {{Content Based Approaches}} to a {{Socially Significant Music Event}}},
  author = {Yadati, Karthik and Larson, Martha and Liem, Cynthia C S and Hanjalic, Alan},
  year = {2014},
  journal = {ISMIR},
  abstract = {Electronic dance music (EDM) is a popular genre of music. In this paper, we propose a method to automatically detect the characteristic event in an EDM recording that is referred to as a drop. Its importance is reflected in the number of users who leave comments in the general neighborhood of drop events in music on online audio distribution platforms like SoundCloud. The variability that characterizes realizations of drop events in EDM makes automatic drop detection challenging. We propose a two-stage approach to drop detection that first models the sound characteristics during drop events and then incorporates temporal structure by zeroing in on a watershed moment. We also explore the possibility of using the drop-related social comments on the SoundCloud platform as weak reference labels to improve drop detection. The method is evaluated using data from SoundCloud. Performance is measured as the overlap between tolerance windows centered around the hypothesized and the actual drop. Initial experimental results are promising, revealing the potential of the proposed method for combining content analysis and social activity to detect events in music recordings.},
  langid = {english},
  keywords = {unread},
  file = {/home/etiandre/stage/biblio/YadatiLarsonLiemHanjalic-ismir2014-Detecting drops in electronic dance music 000297.pdf}
}

@article{yangAligningUnsynchronizedPart2021,
  title = {Aligning {{Unsynchronized Part Recordings}} to a {{Full Mix Using Iterative Subtractive Alignment}}},
  author = {Yang, Daniel and Ji, Kevin and Tsai, Tj},
  year = {2021},
  journal = {ISMIR},
  abstract = {This paper explores an application that would enable a group of musicians in quarantine to produce a performance of a chamber work by recording each part in isolation in a completely unsynchronized manner, and then generating a synchronized performance by aligning, time scale modifying, and mixing the individual part recordings. We focus on the main technical challenge of aligning the individual part recordings against a reference ``full mix'' recording containing a performance of the work. We propose an iterative subtractive alignment approach, in which each part recording is aligned against the full mix recording and then subtracted from it. We also explore different feature representations and cost metrics to handle the asymmetrical nature of the part--full mix comparison. We evaluate our proposed approach on two different datasets: one that is a modification of the URMP dataset that presents an idealized setting, and another that contains a small set of piano trio data collected from musicians during the pandemic specifically for this study. Compared to a standard pairwise alignment approach, we find that the proposed approach has strong performance on the URMP dataset and mixed success on the more realistic piano trio data.},
  langid = {english},
  keywords = {alignment},
  file = {/home/etiandre/stage/biblio/Yang-ismir2021-aligning PianoTrioAlignment.pdf}
}

@misc{zehrenAutomaticDetectionCue2020,
  title = {Automatic {{Detection}} of {{Cue Points}} for {{DJ Mixing}}},
  author = {Zehren, Micka{\"e}l and Alunno, Marco and Bientinesi, Paolo},
  year = {2020},
  month = jul,
  number = {arXiv:2007.08411},
  eprint = {2007.08411},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  urldate = {2024-03-05},
  abstract = {The automatic identification of cue points is a central task in applications as diverse as music thumbnailing, mashups generation, and DJ mixing. Our focus lies in electronic dance music and in specific cue points, the ``switch points'', that make it possible to automatically construct transitions among tracks, mimicking what professional DJs do. We present an approach for the detection of switch points that embody a few general rules we established from interviews with professional DJs; the implementation of these rules is based on features extraction and novelty analysis. The quality of the generated switch points is assessed both by comparing them with a manually annotated dataset that we curated, and by evaluating them individually. We found that about 96\% of the points generated by our methodology are of good quality for use in a DJ mix.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Sound,cue point,Electrical Engineering and Systems Science - Audio and Speech Processing,extraction},
  file = {/home/etiandre/stage/biblio/ZehrenAlunnoBientinesi-arXiv2020-Automatic Detection of Cue Points for DJ Mixing 2007.08411.pdf}
}

@article{zehrenMDJCUEMANUALLYANNOTATED2019,
  title = {M-{{DJCUE}}: {{A MANUALLY ANNOTATED DATASET OF CUE POINTS}}},
  author = {Zehren, Micka{\"e}l and Alunno, Marco and Bientinesi, Paolo},
  year = {2019},
  langid = {english},
  keywords = {dataset},
  file = {/home/etiandre/stage/biblio/Zehren-ismir-lbd2019-M-DJCUE - A MANUALLY ANNOTATED DATASET OF CUE POINTS 000025.pdf;/home/etiandre/Zotero/storage/67ERR5QW/Zehren et al. - 2019 - M-DJCUE A MANUALLY ANNOTATED DATASET OF CUE POINTS.pdf;/home/etiandre/Zotero/storage/R9JTFLJW/Zehren et al. - M-DJCUE A Manually Annotated Dataset of Cue Points.pdf;/home/etiandre/Zotero/storage/YDVK7G69/Zehren et al. - 2019 - M-DJCUE A MANUALLY ANNOTATED DATASET OF CUE POINTS.pdf}
}
